# Agent API
Source: https://docs.agno.com/agent-api/introduction

A robust, production-ready application for serving Agents as an API.

Welcome to the Simple Agent API: a robust, production-ready application for serving Agents as an API. It includes:

* A FastAPI server for handling API requests.
* A PostgreSQL database for storing Agent sessions, knowledge, and memories.
* A set of pre-built Agents to use as a starting point.

<Snippet file="simple-agent-api-setup.mdx" />

<Snippet file="create-simple-agent-api-codebase.mdx" />

<Snippet file="simple-agent-api-dependency-management.mdx" />

<Snippet file="simple-agent-api-production.mdx" />

## Additional Information

Congratulations on running your  Agent API.

* Read how to [use workspaces with your Agent API](/workspaces/introduction)


# A beautiful UI for your Agents
Source: https://docs.agno.com/agent-ui/introduction

A beautiful, open-source interface for interacting with AI agents

<Frame>
  <img height="200" src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=2ac21c49e63cb721e47d850de3a16dfa" style={{ borderRadius: '8px' }} width="1788" height="936" data-path="images/agent-ui.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?w=280&maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=19a4278316a2d20edda1955b380f8e94 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?w=560&maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=f24fea4df5b5c48a559d936bd999531f 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?w=840&maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=004ec817fa2c1109af208e54f536cbf8 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?w=1100&maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=edfbc774026679322648cec6c31d84ce 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?w=1650&maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=516418b211c22f73945a3f0a15cf2620 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui.png?w=2500&maxW=1788&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=33b3c7a3ca53ad48163efb2c56cc9692 2500w" data-optimize="true" data-opv="2" />
</Frame>

Agno provides a beautiful UI for interacting with your agents, completely open source, free to use and build on top of. It's a simple interface that allows you to chat with your agents, view their memory, knowledge, and more.

<Note>
  No data is sent to [agno.com](https://app.agno.com), all agent data is stored locally in your sqlite database.
</Note>

The Open Source Agent UI is built with Next.js and TypeScript. After the success of the [Agent Playground](/introduction/playground), the community asked for a self-hosted alternative and we delivered!

# Get Started with Agent UI

To clone the Agent UI, run the following command in your terminal:

```bash
npx create-agent-ui@latest
```

Enter `y` to create a new project, install dependencies, then run the agent-ui using:

```bash
cd agent-ui && npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to view the Agent UI, but remember to connect to your local agents.

<Frame>
  <img height="200" src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=daebe1abaf86b143c9cfcabaf32a7a72" style={{ borderRadius: '8px' }} width="2892" height="1616" data-path="images/agent-ui-homepage.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?w=280&maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=4e507b28e357108977ea5ffe5de0ef2b 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?w=560&maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=debd33c21875cada22a9f272c627da4a 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?w=840&maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=cf0784f29a9fc9ec6b10279019afaa7a 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?w=1100&maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=08fa68e1460a9405ffd1d0d48c34559e 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?w=1650&maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=d3476ca56a89baa19999f0d51cda38a4 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-ui-homepage.png?w=2500&maxW=2892&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=548c52a4ae23591081f6124b93791804 2500w" data-optimize="true" data-opv="2" />
</Frame>

<br />

<Accordion title="Clone the repository manually" icon="github">
  You can also clone the repository manually

  ```bash
  git clone https://github.com/agno-agi/agent-ui.git
  ```

  And run the agent-ui using

  ```bash
  cd agent-ui && pnpm install && pnpm dev
  ```
</Accordion>

## Connect to Local Agents

The Agent UI needs to connect to a playground server, which you can run locally or on any cloud provider.

Let's start with a local playground server. Create a file `playground.py`

```python playground.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.sqlite import SqliteStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

agent_storage: str = "tmp/agents.db"

web_agent = Agent(
    name="Web Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    instructions=["Always include sources"],
    # Store the agent sessions in a sqlite database
    storage=SqliteStorage(table_name="web_agent", db_file=agent_storage),
    # Adds the current date and time to the instructions
    add_datetime_to_instructions=True,
    # Adds the history of the conversation to the messages
    add_history_to_messages=True,
    # Number of history responses to add to the messages
    num_history_responses=5,
    # Adds markdown formatting to the messages
    markdown=True,
)

finance_agent = Agent(
    name="Finance Agent",
    model=OpenAIChat(id="gpt-4o"),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True, company_news=True)],
    instructions=["Always use tables to display data"],
    storage=SqliteStorage(table_name="finance_agent", db_file=agent_storage),
    add_datetime_to_instructions=True,
    add_history_to_messages=True,
    num_history_responses=5,
    markdown=True,
)

playground = Playground(agents=[web_agent, finance_agent])
app = playground.get_app()

if __name__ == "__main__":
    playground.serve("playground:app", reload=True)
```

In another terminal, run the playground server:

<Steps>
  <Step title="Setup your virtual environment">
    <CodeGroup>
      ```bash Mac
      python3 -m venv .venv
      source .venv/bin/activate
      ```

      ```bash Windows
      python3 -m venv aienv
      aienv\Scripts\activate
      ```
    </CodeGroup>
  </Step>

  <Step title="Install dependencies">
    <CodeGroup>
      ```bash Mac
      pip install -U openai duckduckgo-search yfinance sqlalchemy 'fastapi[standard]' agno
      ```

      ```bash Windows
      pip install -U openai duckduckgo-search yfinance sqlalchemy 'fastapi[standard]' agno
      ```
    </CodeGroup>
  </Step>

  <Step title="Export your OpenAI key">
    <CodeGroup>
      ```bash Mac
      export OPENAI_API_KEY=sk-***
      ```

      ```bash Windows
      setx OPENAI_API_KEY sk-***
      ```
    </CodeGroup>
  </Step>

  <Step title="Run the Playground">
    ```shell
    python playground.py
    ```
  </Step>
</Steps>

<Tip>Make sure the `serve_playground_app()` points to the file containing your `Playground` app.</Tip>

## View the playground

* Open [http://localhost:3000](http://localhost:3000) to view the Agent UI
* Select the `localhost:7777` endpoint and start chatting with your agents!

<video autoPlay muted controls className="w-full aspect-video" src="https://mintcdn.com/agno/QZOB15dhrj4yAmBd/videos/agent-ui-demo.mp4?auto=format&n=QZOB15dhrj4yAmBd&q=85&s=d66a7566f567b08739ccba456eada81b" data-path="videos/agent-ui-demo.mp4" />


# Agent Context
Source: https://docs.agno.com/agents/context



Agent Context is another amazing feature of Agno. `context` is a dictionary that contains a set of functions (or dependencies) that are resolved before the agent runs.

<Note>
  Context is a way to inject dependencies into the description and instructions of the agent.

  You can use context to inject memories, dynamic few-shot examples, "retrieved" documents, etc.
</Note>

```python agent_context.py
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the context is evaluated when the agent is run,
    # think of it as dependency injection for Agents
    context={"top_hackernews_stories": get_top_hackernews_stories},
    # Alternatively, you can manually add the context to the instructions
    instructions=dedent("""\
        You are an insightful tech trend observer! 📰

        Here are the top stories on HackerNews:
        {top_hackernews_stories}\
    """),
    # add_state_in_messages will make the `top_hackernews_stories` variable
    # available in the instructions
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)
```

## Adding the entire context to the user message

Set `add_context=True` to add the entire context to the user message. This way you don't have to manually add the context to the instructions.

```python agent_context_instructions.py
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.models.openai import OpenAIChat


def get_top_hackernews_stories(num_stories: int = 5) -> str:
    """Fetch and return the top stories from HackerNews.

    Args:
        num_stories: Number of top stories to retrieve (default: 5)
    Returns:
        JSON string containing story details (title, url, score, etc.)
    """
    # Get top stories
    stories = [
        {
            k: v
            for k, v in httpx.get(
                f"https://hacker-news.firebaseio.com/v0/item/{id}.json"
            )
            .json()
            .items()
            if k != "kids"  # Exclude discussion threads
        }
        for id in httpx.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json"
        ).json()[:num_stories]
    ]
    return json.dumps(stories, indent=4)


# Create a Context-Aware Agent that can access real-time HackerNews data
agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    # Each function in the context is resolved when the agent is run,
    # think of it as dependency injection for Agents
    context={"top_hackernews_stories": get_top_hackernews_stories},
    # We can add the entire context dictionary to the instructions
    add_context=True,
    markdown=True,
)

# Example usage
agent.print_response(
    "Summarize the top stories on HackerNews and identify any interesting trends.",
    stream=True,
)
```


# What are Agents?
Source: https://docs.agno.com/agents/introduction

Learn about Agno Agents and how they work.

**Agents** are AI programs that operate autonomously. Traditional software follows a pre-programmed sequence of steps. Agents dynamically determine their course of action using a machine learning **model**.

The core of an Agent is the **model**, **tools** and **instructions**:

* **Model:** controls the flow of execution. It decides whether to reason, act or respond.
* **Tools:** enable an Agent to take actions and interact with external systems.
* **Instructions:** are how we program the Agent, teaching it how to use tools and respond.

Agents also have **memory**, **knowledge**, **storage** and the ability to **reason**:

* **Reasoning:** enables Agents to "think" before responding and "analyze" the results of their actions (i.e. tool calls), this improves reliability and quality of responses.
* **Knowledge:** is domain-specific information that the Agent can **search at runtime** to make better decisions and provide accurate responses (RAG). Knowledge is stored in a vector database and this **search at runtime** pattern is known as Agentic RAG/Agentic Search.
* **Storage:** is used by Agents to save session history and state in a database. Model APIs are stateless and storage enables us to continue conversations from where they left off. This makes Agents stateful, enabling multi-turn, long-term conversations.
* **Memory:** gives Agents the ability to store and recall information from previous interactions, allowing them to learn user preferences and personalize their responses.

<img height="200" src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=4d9aa6cf0fd744f706ce4a380a226cd1" style={{ borderRadius: "8px" }} width="7752" height="4788" data-path="images/agent.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?w=280&maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=8596bac39ad84139bdccb18fc249b923 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?w=560&maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=5aca8ddb00239e284066eec81ec3e9c7 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?w=840&maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=c8e14464d6ed9a3811f188d9e8271f53 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?w=1100&maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=da790f66dbd426edfd5b61ccad67bb79 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?w=1650&maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=9eb137508e887bdb9c90e102db34e35c 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent.png?w=2500&maxW=7752&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=72a72a50512463427e16919f12b4d934 2500w" data-optimize="true" data-opv="2" />

<Check>
  If this is your first time building agents, [follow these examples](/introduction/agents#basic-agent) before diving into advanced concepts.
</Check>

## Example: Research Agent

Let's build a research agent using Exa to showcase how to guide the Agent to produce the report in a specific format. In advanced cases, we should use [Structured Outputs](/agents/structured-output) instead.

<Note>
  The description and instructions are converted to the system message and the
  input is passed as the user message. Set `debug_mode=True` to view logs behind
  the scenes.
</Note>

<Steps>
  <Step title="Create Research Agent">
    Create a file `research_agent.py`

    ```python research_agent.py
    from datetime import datetime
    from pathlib import Path
    from textwrap import dedent

    from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    from agno.tools.exa import ExaTools

    today = datetime.now().strftime("%Y-%m-%d")

    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        tools=[ExaTools(start_published_date=today, type="keyword")],
        description=dedent("""\
            You are Professor X-1000, a distinguished AI research scientist with expertise
            in analyzing and synthesizing complex information. Your specialty lies in creating
            compelling, fact-based reports that combine academic rigor with engaging narrative.

            Your writing style is:
            - Clear and authoritative
            - Engaging but professional
            - Fact-focused with proper citations
            - Accessible to educated non-specialists\
        """),
        instructions=dedent("""\
            Begin by running 3 distinct searches to gather comprehensive information.
            Analyze and cross-reference sources for accuracy and relevance.
            Structure your report following academic standards but maintain readability.
            Include only verifiable facts with proper citations.
            Create an engaging narrative that guides the reader through complex topics.
            End with actionable takeaways and future implications.\
        """),
        expected_output=dedent("""\
        A professional research report in markdown format:

        # {Compelling Title That Captures the Topic's Essence}

        ## Executive Summary
        {Brief overview of key findings and significance}

        ## Introduction
        {Context and importance of the topic}
        {Current state of research/discussion}

        ## Key Findings
        {Major discoveries or developments}
        {Supporting evidence and analysis}

        ## Implications
        {Impact on field/society}
        {Future directions}

        ## Key Takeaways
        - {Bullet point 1}
        - {Bullet point 2}
        - {Bullet point 3}

        ## References
        - [Source 1](link) - Key finding/quote
        - [Source 2](link) - Key finding/quote
        - [Source 3](link) - Key finding/quote

        ---
        Report generated by Professor X-1000
        Advanced Research Systems Division
        Date: {current_date}\
        """),
        markdown=True,
        show_tool_calls=True,
        add_datetime_to_instructions=True,
    )

    # Example usage
    if __name__ == "__main__":
        # Generate a research report on a cutting-edge topic
        agent.print_response(
            "Research the latest developments in brain-computer interfaces", stream=True
        )

    # More example prompts to try:
    """
    Try these research topics:
    1. "Analyze the current state of solid-state batteries"
    2. "Research recent breakthroughs in CRISPR gene editing"
    3. "Investigate the development of autonomous vehicles"
    4. "Explore advances in quantum machine learning"
    5. "Study the impact of artificial intelligence on healthcare"
    """
    ```
  </Step>

  <Step title="Run the agent">
    Install libraries

    ```shell
    pip install openai exa-py agno
    ```

    Run the agent

    ```shell
    python research_agent.py
    ```
  </Step>
</Steps>


# Knowledge
Source: https://docs.agno.com/agents/knowledge



**Knowledge** is domain-specific information that the Agent can **search** at runtime to make better decisions (dynamic few-shot learning) and provide accurate responses (agentic RAG). Knowledge is stored in a vector db and this **searching on demand** pattern is called Agentic RAG.

<Accordion title="Dynamic Few-Shot Learning: Text2Sql Agent" icon="database">
  Example: If we're building a Text2Sql Agent, we'll need to give the table schemas, column names, data types, example queries, common "gotchas" to help it generate the best-possible SQL query.

  We're obviously not going to put this all in the system prompt, instead we store this information in a vector database and let the Agent query it at runtime.

  Using this information, the Agent can then generate the best-possible SQL query. This is called dynamic few-shot learning.
</Accordion>

**Agno Agents use Agentic RAG** by default, meaning when we provide `knowledge` to an Agent, it will search this knowledge base, at runtime, for the specific information it needs to achieve its task.

The pseudo steps for adding knowledge to an Agent are:

```python
from agno.agent import Agent, AgentKnowledge

# Create a knowledge base for the Agent
knowledge_base = AgentKnowledge(vector_db=...)

# Add information to the knowledge base
knowledge_base.load_text("The sky is blue")

# Add the knowledge base to the Agent and
# give it a tool to search the knowledge base as needed
agent = Agent(knowledge=knowledge_base, search_knowledge=True)
```

We can give our agent access to the knowledge base in the following ways:

* We can set `search_knowledge=True` to add a `search_knowledge_base()` tool to the Agent. `search_knowledge` is `True` **by default** if you add `knowledge` to an Agent.
* We can set `add_references=True` to automatically add references from the knowledge base to the Agent's prompt. This is the traditional 2023 RAG approach.

<Tip>
  If you need complete control over the knowledge base search, you can pass your own `retriever` function with the following signature:

  ```python
  def retriever(agent: Agent, query: str, num_documents: Optional[int], **kwargs) -> Optional[list[dict]]:
    ...
  ```

  This function is called during `search_knowledge_base()` and is used by the Agent to retrieve references from the knowledge base.
</Tip>

## Vector Databases

While any type of storage can act as a knowledge base, vector databases offer the best solution for retrieving relevant results from dense information quickly. Here's how vector databases are used with Agents:

<Steps>
  <Step title="Chunk the information">
    Break down the knowledge into smaller chunks to ensure our search query
    returns only relevant results.
  </Step>

  <Step title="Load the knowledge base">
    Convert the chunks into embedding vectors and store them in a vector
    database.
  </Step>

  <Step title="Search the knowledge base">
    When the user sends a message, we convert the input message into an
    embedding and "search" for nearest neighbors in the vector database.
  </Step>
</Steps>

<Note>
  Knowledge filters are currently supported on the following knowledge base types: <b>PDF</b>, <b>PDF\_URL</b>, <b>Text</b>, <b>JSON</b>, and <b>DOCX</b>.
  For more details, see the [Knowledge Filters documentation](/filters/introduction).
</Note>

## Example: RAG Agent with a PDF Knowledge Base

Let's build a **RAG Agent** that answers questions from a PDF.

### Step 1: Run PgVector

Let's use `PgVector` as our vector db as it can also provide storage for our Agents.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

```bash
docker run -d \
  -e POSTGRES_DB=ai \
  -e POSTGRES_USER=ai \
  -e POSTGRES_PASSWORD=ai \
  -e PGDATA=/var/lib/postgresql/data/pgdata \
  -v pgvolume:/var/lib/postgresql/data \
  -p 5532:5432 \
  --name pgvector \
  agnohq/pgvector:16
```

### Step 2: Traditional RAG

Retrieval Augmented Generation (RAG) means **"stuffing the prompt with relevant information"** to improve the model's response. This is a 2 step process:

1. Retrieve relevant information from the knowledge base.
2. Augment the prompt to provide context to the model.

Let's build a **traditional RAG** Agent that answers questions from a PDF of recipes.

<Steps>
  <Step title="Install libraries">
    Install the required libraries using pip

    <CodeGroup>
      ```bash Mac
      pip install -U pgvector pypdf "psycopg[binary]" sqlalchemy
      ```

      ```bash Windows
      pip install -U pgvector pypdf "psycopg[binary]" sqlalchemy
      ```
    </CodeGroup>
  </Step>

  <Step title="Create a Traditional RAG Agent">
    Create a file `traditional_rag.py` with the following contents

    ```python traditional_rag.py
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
    from agno.vectordb.pgvector import PgVector, SearchType

    db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
    knowledge_base = PDFUrlKnowledgeBase(
        # Read PDF from this URL
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        # Store embeddings in the `ai.recipes` table
        vector_db=PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.hybrid),
    )
    # Load the knowledge base: Comment after first run
    knowledge_base.load(upsert=True)

    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        knowledge=knowledge_base,
        # Enable RAG by adding references from AgentKnowledge to the user prompt.
        add_references=True,
        # Set as False because Agents default to `search_knowledge=True`
        search_knowledge=False,
        markdown=True,
        # debug_mode=True,
    )
    agent.print_response("How do I make chicken and galangal in coconut milk soup")
    ```
  </Step>

  <Step title="Run the agent">
    Run the agent (it takes a few seconds to load the knowledge base).

    <CodeGroup>
      ```bash Mac
      python traditional_rag.py
      ```

      ```bash Windows
      python traditional_rag.py
      ```
    </CodeGroup>

    <br />
  </Step>
</Steps>

<Accordion title="How to use local PDFs" icon="file-pdf" iconType="duotone">
  If you want to use local PDFs, use a `PDFKnowledgeBase` instead

  ```python agent.py
  from agno.knowledge.pdf import PDFKnowledgeBase

  ...
  knowledge_base = PDFKnowledgeBase(
      path="data/pdfs",
      vector_db=PgVector(
          table_name="pdf_documents",
          db_url=db_url,
      ),
  )
  ...
  ```
</Accordion>

### Step 3: Agentic RAG

With traditional RAG above, `add_references=True` always adds information from the knowledge base to the prompt, regardless of whether it is relevant to the question or helpful.

With Agentic RAG, we let the Agent decide **if** it needs to access the knowledge base and what search parameters it needs to query the knowledge base.

Set `search_knowledge=True` and `read_chat_history=True`, giving the Agent tools to search its knowledge and chat history on demand.

<Steps>
  <Step title="Create an Agentic RAG Agent">
    Create a file `agentic_rag.py` with the following contents

    ```python agentic_rag.py
    from agno.agent import Agent
    from agno.models.openai import OpenAIChat
    from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
    from agno.vectordb.pgvector import PgVector, SearchType

    db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.hybrid),
    )
    # Load the knowledge base: Comment out after first run
    knowledge_base.load(upsert=True)

    agent = Agent(
        model=OpenAIChat(id="gpt-4o"),
        knowledge=knowledge_base,
        # Add a tool to search the knowledge base which enables agentic RAG.
        search_knowledge=True,
        # Add a tool to read chat history.
        read_chat_history=True,
        show_tool_calls=True,
        markdown=True,
        # debug_mode=True,
    )
    agent.print_response("How do I make chicken and galangal in coconut milk soup", stream=True)
    agent.print_response("What was my last question?", markdown=True)
    ```
  </Step>

  <Step title="Run the agent">
    Run the agent

    <CodeGroup>
      ```bash Mac
      python agentic_rag.py
      ```

      ```bash Windows
      python agentic_rag.py
      ```
    </CodeGroup>

    <Note>
      Notice how it searches the knowledge base and chat history when needed
    </Note>
  </Step>
</Steps>

## Attributes

| Parameter                  | Type                                  | Default | Description                                                                                                                                                                                                 |
| -------------------------- | ------------------------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `knowledge`                | `AgentKnowledge`                      | `None`  | Provides the knowledge base used by the agent.                                                                                                                                                              |
| `search_knowledge`         | `bool`                                | `True`  | Adds a tool that allows the Model to search the knowledge base (aka Agentic RAG). Enabled by default when `knowledge` is provided.                                                                          |
| `add_references`           | `bool`                                | `False` | Enable RAG by adding references from AgentKnowledge to the user prompt.                                                                                                                                     |
| `retriever`                | `Callable[..., Optional[list[dict]]]` | `None`  | Function to get context to add to the user message. This function is called when add\_references is True.                                                                                                   |
| `context_format`           | `Literal['json', 'yaml']`             | `json`  | Specifies the format for RAG, either "json" or "yaml".                                                                                                                                                      |
| `add_context_instructions` | `bool`                                | `False` | If True, add instructions for using the context to the system prompt (if knowledge is also provided). For example: add an instruction to prefer information from the knowledge base over its training data. |

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agent_concepts/knowledge)


# Memory
Source: https://docs.agno.com/agents/memory



Memory gives an Agent the ability to recall relavant information. Memory is a part of the Agent's context that helps it provide the best, most personalized response.

<Check>
  If the user tells the Agent they like to ski, then future responses can reference this information to provide a more personalized experience.
</Check>

In Agno, Memory covers chat history, user preferences and any supplemental information about the task at hand. **Agno supports 3 types of memory out of the box:**

1. **Session Storage (chat history and session state):** Session storage saves an Agent's sessions in a database and enables Agents to have multi-turn conversations. Session storage also holds the session state, which is persisted across runs because it is saved to the database after each run. Session storage is a form of short-term memory **called "Storage" in Agno**.

2. **User Memories (user preferences):** The Agent can store insights and facts about the user that it learns through conversation. This helps the agents personalize its response to the user it is interacting with. Think of this as adding "ChatGPT like memory" to your agent. **This is called "Memory" in Agno**.

3. **Session Summaries (chat summary):** The Agent can store a condensed representations of the session, useful when chat histories gets too long. **This is called "Summary" in Agno**.

<Note>
  It is relatively easy to use your own memory implementation using `Agent.context`.
</Note>

To become an expert in Agentic Memory, you need to learn about:

1. [Default, built-in Memory](/agents/memory#default-memory)
2. [Session Storage](/agents/memory#session-storage)
3. [User Memories](/agents/memory#user-memories)
4. [Session Summaries](/agents/memory#session-summaries)

## Show me the code: Memory & Storage in Action

Here's a simple but complete example of using Memory and Storage in an Agent.

```python memory_demo.py
from agno.agent import Agent
from agno.memory.v2.db.sqlite import SqliteMemoryDb
from agno.memory.v2.memory import Memory
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich.pretty import pprint

# UserId for the memories
user_id = "ava"
# Database file for memory and storage
db_file = "tmp/agent.db"

# Initialize memory.v2
memory = Memory(
    # Use any model for creating memories
    model=OpenAIChat(id="gpt-4.1"),
    db=SqliteMemoryDb(table_name="user_memories", db_file=db_file),
)
# Initialize storage
storage = SqliteStorage(table_name="agent_sessions", db_file=db_file)

# Initialize Agent
memory_agent = Agent(
    model=OpenAIChat(id="gpt-4.1"),
    # Store memories in a database
    memory=memory,
    # Give the Agent the ability to update memories
    enable_agentic_memory=True,
    # OR - Run the MemoryManager after each response
    enable_user_memories=True,
    # Store the chat history in the database
    storage=storage,
    # Add the chat history to the messages
    add_history_to_messages=True,
    # Number of history runs
    num_history_runs=3,
    markdown=True,
)

memory.clear()
memory_agent.print_response(
    "My name is Ava and I like to ski.",
    user_id=user_id,
    stream=True,
    stream_intermediate_steps=True,
)
print("Memories about Ava:")
pprint(memory.get_user_memories(user_id=user_id))

memory_agent.print_response(
    "I live in san francisco, where should i move within a 4 hour drive?",
    user_id=user_id,
    stream=True,
    stream_intermediate_steps=True,
)
print("Memories about Ava:")
pprint(memory.get_user_memories(user_id=user_id))
```

### Notes

* `enable_agentic_memory=True` gives the Agent a tool to manage memories of the user, this tool passes the task to the `MemoryManager` class. You may also set `enable_user_memories=True` which always runs the `MemoryManager` after each user message.
* `add_history_to_messages=True` adds the chat history to the messages sent to the Model, the `num_history_runs` determines how many runs to add.
* `read_chat_history=True` adds a tool to the Agent that allows it to read chat history, as it may be larger than what's included in the `num_history_runs`.

## Default Memory

Every Agent comes with built-in memory which keeps track of the messages in the session i.e. the chat history.

You can access these messages using `agent.get_messages_for_session()`.

We can give the Agent access to the chat history in the following ways:

* We can set `add_history_to_messages=True` and `num_history_runs=5` to add the messages from the last 5 runs automatically to every message sent to the agent.
* We can set `read_chat_history=True` to provide a `get_chat_history()` tool to your agent allowing it to read any message in the entire chat history.
* **We recommend setting all 3: `add_history_to_messages=True`, `num_history_runs=3` and `read_chat_history=True` for the best experience.**
* We can also set `read_tool_call_history=True` to provide a `get_tool_call_history()` tool to your agent allowing it to read tool calls in reverse chronological order.

<Note>
  The default memory is not persisted across execution cycles. So after the script finishes running, or the request is over, the built-in default memory is lost.

  You can persist this memory in a database by adding a `storage` driver to the Agent.
</Note>

<Steps>
  <Step title="Built-in memory example">
    ```python agent_memory.py
    from agno.agent import Agent
    from agno.models.google.gemini import Gemini
    from rich.pretty import pprint

    agent = Agent(
        model=Gemini(id="gemini-2.0-flash-exp"),
        # Set add_history_to_messages=true to add the previous chat history to the messages sent to the Model.
        add_history_to_messages=True,
        # Number of historical responses to add to the messages.
        num_history_responses=3,
        description="You are a helpful assistant that always responds in a polite, upbeat and positive manner.",
    )

    # -*- Create a run
    agent.print_response("Share a 2 sentence horror story", stream=True)
    # -*- Print the messages in the memory
    pprint([m.model_dump(include={"role", "content"}) for m in agent.get_messages_for_session()])

    # -*- Ask a follow up question that continues the conversation
    agent.print_response("What was my first message?", stream=True)
    # -*- Print the messages in the memory
    pprint([m.model_dump(include={"role", "content"}) for m in agent.get_messages_for_session()])
    ```
  </Step>

  <Step title="Run the example">
    Install libraries

    ```shell
    pip install google-genai agno
    ```

    Export your key

    ```shell
    export GOOGLE_API_KEY=xxx
    ```

    Run the example

    ```shell
    python agent_memory.py
    ```
  </Step>
</Steps>

## Session Storage

The built-in memory is only available during the current execution cycle. Once the script ends, or the request is over, the built-in memory is lost.

**Storage** help us save Agent sessions and state to a database or file.

Adding storage to an Agent is as simple as providing a `storage` driver and Agno handles the rest. You can use Sqlite, Postgres, Mongo or any other database you want.

Here's a simple example that demonstrates persistence across execution cycles:

```python storage.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Fix the session id to continue the same session across execution cycles
    session_id="fixed_id_for_demo",
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/data.db"),
    add_history_to_messages=True,
    num_history_runs=3,
)
agent.print_response("What was my last question?")
agent.print_response("What is the capital of France?")
agent.print_response("What was my last question?")
pprint(agent.get_messages_for_session())
```

The first time you run this, the answer to "What was my last question?" will not be available. But run it again and the Agent will able to answer properly. Because we have fixed the session id, the Agent will continue from the same session every time you run the script.

Read more in the [storage](/agents/storage) section.

## User Memories

Along with storing session history and state, Agents can also create user memories based on the conversation history.

To enable user memories, give your Agent a `Memory` object and set `enable_agentic_memory=True`.

<Note>
  Enabling agentic memory will also add all existing user memories to the agent's system prompt.
</Note>

<Steps>
  <Step title="User memory example">
    ```python user_memory.py
    from agno.agent import Agent
    from agno.memory.v2.db.sqlite import SqliteMemoryDb
    from agno.memory.v2.memory import Memory
    from agno.models.google.gemini import Gemini

    memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
    memory = Memory(db=memory_db)

    john_doe_id = "john_doe@example.com"

    agent = Agent(
        model=Gemini(id="gemini-2.0-flash-exp"),
        memory=memory,
        enable_agentic_memory=True,
    )

    # The agent can add new memories to the user's memory
    agent.print_response(
        "My name is John Doe and I like to hike in the mountains on weekends.",
        stream=True,
        user_id=john_doe_id,
    )

    agent.print_response("What are my hobbies?", stream=True, user_id=john_doe_id)

    # The agent can also remove all memories from the user's memory
    agent.print_response(
        "Remove all existing memories of me. Completely clear the DB.",
        stream=True,
        user_id=john_doe_id,
    )

    agent.print_response(
        "My name is John Doe and I like to paint.", stream=True, user_id=john_doe_id
    )

    # The agent can remove specific memories from the user's memory
    agent.print_response("Remove any memory of my name.", stream=True, user_id=john_doe_id)

    ```
  </Step>

  <Step title="Run the example">
    Install libraries

    ```shell
    pip install google-genai agno
    ```

    Export your key

    ```shell
    export GOOGLE_API_KEY=xxx
    ```

    Run the example

    ```shell
    python user_memory.py
    ```
  </Step>
</Steps>

User memories are stored in the `Memory` object and persisted in the `SqliteMemoryDb` to be used across multiple users and multiple sessions.

## Session Summaries

To enable session summaries, set `enable_session_summaries=True` on the `Agent`.

<Steps>
  <Step title="Session summary example">
    ```python session_summary.py
        from agno.agent import Agent
        from agno.memory.v2.db.sqlite import SqliteMemoryDb
        from agno.memory.v2.memory import Memory
        from agno.models.google.gemini import Gemini

        memory_db = SqliteMemoryDb(table_name="memory", db_file="tmp/memory.db")
        memory = Memory(db=memory_db)

        user_id = "jon_hamm@example.com"
        session_id = "1001"

        agent = Agent(
            model=Gemini(id="gemini-2.0-flash-exp"),
            memory=memory,
            enable_session_summaries=True,
        )

        agent.print_response(
            "What can you tell me about quantum computing?",
            stream=True,
            user_id=user_id,
            session_id=session_id,
        )

        agent.print_response(
            "I would also like to know about LLMs?",
            stream=True,
            user_id=user_id,
            session_id=session_id
        )

        session_summary = memory.get_session_summary(
            user_id=user_id, session_id=session_id
        )
        print(f"Session summary: {session_summary.summary}\n")
    ```
  </Step>

  <Step title="Run the example">
    Install libraries

    ```shell
    pip install google-genai agno
    ```

    Export your key

    ```shell
    export GOOGLE_API_KEY=xxx
    ```

    Run the example

    ```shell
    python session_summary.py
    ```
  </Step>
</Steps>

## Attributes

| Parameter                  | Type     | Default    | Description                                                                                                     |
| -------------------------- | -------- | ---------- | --------------------------------------------------------------------------------------------------------------- |
| `memory`                   | `Memory` | `Memory()` | Agent's memory object used for storing and retrieving information.                                              |
| `add_history_to_messages`  | `bool`   | `False`    | If true, adds the chat history to the messages sent to the Model. Also known as `add_chat_history_to_messages`. |
| `num_history_runs`         | `int`    | `3`        | Number of historical responses to add to the messages.                                                          |
| `enable_user_memories`     | `bool`   | `False`    | If true, create and store personalized memories for the user.                                                   |
| `enable_session_summaries` | `bool`   | `False`    | If true, create and store session summaries.                                                                    |
| `enable_agentic_memory`    | `bool`   | `False`    | If true, enables the agent to manage the user's memory.                                                         |

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agent_concepts/memory)
* View [Examples](/examples/concepts/memory)


# Metrics
Source: https://docs.agno.com/agents/metrics

Understanding agent run and session metrics in Agno

## Overview

When you run an agent in Agno, the response you get (**RunResponse**) includes detailed metrics about the run. These metrics help you understand resource usage (like **token usage** and **time**), performance, and other aspects of the model and tool calls.

Metrics are available at multiple levels:

* **Per-message**: Each message (assistant, tool, etc.) has its own metrics.
* **Per-tool call**: Each tool execution has its own metrics.
* **Aggregated**: The `RunResponse` aggregates metrics across all messages in the run.

<Note>
  Where Metrics Live

  * `RunResponse.metrics`: Aggregated metrics for the whole run, as a dictionary.
  * `ToolExecution.metrics`: Metrics for each tool call.
  * `Message.metrics`: Metrics for each message (assistant, tool, etc.).
</Note>

## Example Usage

Suppose you have an agent that performs some tasks and you want to analyze the metrics after running it. Here's how you can access and print the metrics:

You run the following code to create an agent and run it with the following configuration:

```python
from typing import Iterator

from agno.agent import Agent, RunResponse
from agno.models.google import Gemini
from agno.tools.yfinance import YFinanceTools
from rich.pretty import pprint

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    tools=[YFinanceTools(stock_price=True)],
    markdown=True,
    show_tool_calls=True,
)

agent.print_response(
    "What is the stock price of NVDA", stream=True
)

# Print metrics per message
if agent.run_response.messages:
    for message in agent.run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

# Print the aggregated metrics for the whole run
print("---" * 5, "Collected Metrics", "---" * 5)
pprint(agent.run_response.metrics)
# Print the aggregated metrics for the whole session
print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.session_metrics)
```

You'd see the outputs with following information:

### Tool Execution Metrics

This section provides metrics for each tool execution. It includes details about the resource usage and performance of individual tool calls.

<img src="https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=2dcf874ebcc7415935ec98e13bb3283c" alt="Tool Run Message Metrics" width="1462" height="760" data-path="images/tools-run-message-metrics.png" srcset="https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?w=280&maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=28ba7209227f60c90a526ac82827b80e 280w, https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?w=560&maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=797ef9d86e0092608229434266a67374 560w, https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?w=840&maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=6ba0bf368085511f4b8ba599eff5d48f 840w, https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?w=1100&maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=907dc0de42e8eca36d547219cf246477 1100w, https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?w=1650&maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=242a1479ee71c695b710474a303ae1cb 1650w, https://mintcdn.com/agno/QZOB15dhrj4yAmBd/images/tools-run-message-metrics.png?w=2500&maxW=1462&auto=format&n=QZOB15dhrj4yAmBd&q=85&s=544ec14eca728e06dcfd4bb2a176ffca 2500w" data-optimize="true" data-opv="2" />

### Message Metrics

Here, you can see the metrics for each message response from the agent. All "assistant" responses will have metrics like this, helping you understand the performance and resource usage at the message level.

<img src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=26c574a125a1048db4d79537ac454bbb" alt="Agent Run Message Metrics" width="1036" height="804" data-path="images/agent-run-message-metrics.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?w=280&maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=6eb06df9948ffd1e81d83f8b903789c7 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?w=560&maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=46a5c1982074e025e3d6aae885b64374 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?w=840&maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=b1ea4c86755516c50ba5bfe4afe68482 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?w=1100&maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=bcbc27143edaafea965110f3b6007d67 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?w=1650&maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=9d39ba97a75124a418e2fbbc8c36226e 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-message-metrics.png?w=2500&maxW=1036&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=047a660b3be74df4662de2894e498074 2500w" data-optimize="true" data-opv="2" />

### Aggregated Run Metrics

The aggregated metrics provide a comprehensive view of the entire run. This includes a summary of all messages and tool calls, giving you an overall picture of the agent's performance and resource usage.

<img src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=f42e4fcee9e5b295b1b18adbb9b97c6c" alt="Aggregated Run Metrics" width="1158" height="550" data-path="images/agent-run-aggregated-metrics.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?w=280&maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=efb0f35dbc8420efacded7b1e591bf57 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?w=560&maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=380ee02b220e1cedff31d2e78841cda5 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?w=840&maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=859fad0c0ba2407555d6245b8c6c245d 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?w=1100&maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=83b5040aeb599cc365b6a593cc56225d 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?w=1650&maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=f87c31fda5b80b32ab08ffc65f053416 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agent-run-aggregated-metrics.png?w=2500&maxW=1158&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=1cc43b6fdf3f49585aa2b3dc3fd4efe9 2500w" data-optimize="true" data-opv="2" />

Similarly for the session metrics, you can see the aggregated metrics across all runs in the session, providing insights into the overall performance and resource usage of the agent across multiple runs.

## How Metrics Are Aggregated

* **Per-message**: Each message (assistant, tool, etc.) has its own metrics object.
* **Run-level**: RunResponse.metrics is a dictionary where each key (e.g., input\_tokens) maps to a list of values from all assistant messages in the run.
* **Session-level**: `SessionMetrics` (see `agent.session_metrics`) aggregates metrics across all runs in the session.

## `MessageMetrics` Params

<Snippet file="message_metrics_params.mdx" />


# Multimodal Agents
Source: https://docs.agno.com/agents/multimodal



Agno agents support text, image, audio and video inputs and can generate text, image, audio and video outputs. For a complete overview, please checkout the [compatibility matrix](/models/compatibility).

## Multimodal inputs to an agent

Let's create an agent that can understand images and make tool calls as needed

### Image Agent

```python image_agent.py
from agno.agent import Agent
from agno.media import Image
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)
```

Run the agent:

```shell
python image_agent.py
```

Similar to images, you can also use audio and video as an input.

### Audio Agent

```python audio_agent.py
import base64

import requests
from agno.agent import Agent, RunResponse  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

agent = Agent(
    model=OpenAIChat(id="gpt-4o-audio-preview", modalities=["text"]),
    markdown=True,
)
agent.print_response(
    "What is in this audio?", audio=[Audio(content=wav_data, format="wav")]
)
```

### Video Agent

<Note>Currently Agno only supports video as an input for Gemini models.</Note>

```python video_agent.py
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

# Please download "GreatRedSpot.mp4" using
# wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4
video_path = Path(__file__).parent.joinpath("GreatRedSpot.mp4")

agent.print_response("Tell me about this video", videos=[Video(filepath=video_path)])
```

## Multimodal outputs from an agent

Similar to providing multimodal inputs, you can also get multimodal outputs from an agent.

### Image Generation

The following example demonstrates how to generate an image using DALL-E with an agent.

```python image_agent.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.dalle import DalleTools

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[DalleTools()],
    description="You are an AI agent that can generate images using DALL-E.",
    instructions="When the user asks you to create an image, use the `create_image` tool to create the image.",
    markdown=True,
    show_tool_calls=True,
)

image_agent.print_response("Generate an image of a white siamese cat")

images = image_agent.get_images()
if images and isinstance(images, list):
    for image_response in images:
        image_url = image_response.url
        print(image_url)
```

### Audio Response

The following example demonstrates how to obtain both text and audio responses from an agent. The agent will respond with text and audio bytes that can be saved to a file.

```python audio_agent.py
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
    ),
    markdown=True,
)
response: RunResponse = agent.run("Tell me a 5 second scary story")

# Save the response audio to a file
if response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/scary_story.wav"
    )
```

## Multimodal inputs and outputs together

You can create Agents that can take multimodal inputs and return multimodal outputs. The following example demonstrates how to provide a combination of audio and text inputs to an agent and obtain both text and audio outputs.

### Audio input and Audio output

```python audio_agent.py
import base64

import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

# Fetch the audio file and convert it to a base64 encoded string
url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

agent = Agent(
    model=OpenAIChat(
        id="gpt-4o-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
    ),
    markdown=True,
)

agent.run("What's in these recording?", audio=[Audio(content=wav_data, format="wav")])

if agent.run_response.response_audio is not None:
    write_audio_to_file(
        audio=agent.run_response.response_audio.content, filename="tmp/result.wav"
    )
```


# Prompts
Source: https://docs.agno.com/agents/prompts



We prompt Agents using `description` and `instructions` and a number of other settings. These settings are used to build the **system** message that is sent to the language model.

Understanding how these prompts are created will help you build better Agents.

The 2 key parameters are:

1. **Description**: A description that guides the overall behaviour of the agent.
2. **Instructions**: A list of precise, task-specific instructions on how to achieve its goal.

<Note>
  Description and instructions only provide a formatting benefit, we do not alter or abstract any information and you can always set the `system_message` to provide your own system prompt.
</Note>

## System message

The system message is created using `description`, `instructions` and a number of other settings. The `description` is added to the start of the system message and `instructions` are added as a list after `Instructions`. For example:

```python instructions.py
from agno.agent import Agent

agent = Agent(
    description="You are a famous short story writer asked to write for a magazine",
    instructions=["You are a pilot on a plane flying from Hawaii to Japan."],
    markdown=True,
    debug_mode=True,
)
agent.print_response("Tell me a 2 sentence horror story.", stream=True)
```

Will translate to (set `debug_mode=True` to view the logs):

```js
DEBUG    ============== system ==============
DEBUG    You are a famous short story writer asked to write for a magazine

         ## Instructions
         - You are a pilot on a plane flying from Hawaii to Japan.
         - Use markdown to format your answers.
DEBUG    ============== user ==============
DEBUG    Tell me a 2 sentence horror story.
DEBUG    ============== assistant ==============
DEBUG    As the autopilot disengaged inexplicably mid-flight over the Pacific, the pilot glanced at the copilot's seat
         only to find it empty despite his every recall of a full crew boarding. Hands trembling, he looked into the
         cockpit's rearview mirror and found his own reflection grinning back with blood-red eyes, whispering,
         "There's no escape, not at 30,000 feet."
DEBUG    **************** METRICS START ****************
DEBUG    * Time to first token:         0.4518s
DEBUG    * Time to generate response:   1.2594s
DEBUG    * Tokens per second:           63.5243 tokens/s
DEBUG    * Input tokens:                59
DEBUG    * Output tokens:               80
DEBUG    * Total tokens:                139
DEBUG    * Prompt tokens details:       {'cached_tokens': 0}
DEBUG    * Completion tokens details:   {'reasoning_tokens': 0}
DEBUG    **************** METRICS END ******************
```

## Set the system message directly

You can manually set the system message using the `system_message` parameter.

```python
from agno.agent import Agent

agent = Agent(system_message="Share a 2 sentence story about")
agent.print_response("Love in the year 12000.")
```

<Tip>
  Some models via some model providers, like `llama-3.2-11b-vision-preview` on Groq, require no system message with other messages. To remove the system message, set `create_default_system_message=False` and `system_message=None`. Additionally, if `markdown=True` is set, it will add a system message, so either remove it or explicitly disable the system message.
</Tip>

## User message

The input `message` sent to the `Agent.run()` or `Agent.print_response()` functions is used as the user message.

## Default system message

The Agent creates a default system message that can be customized using the following parameters:

| Parameter                       | Type        | Default  | Description                                                                                                                                                             |
| ------------------------------- | ----------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `description`                   | `str`       | `None`   | A description of the Agent that is added to the start of the system message.                                                                                            |
| `goal`                          | `str`       | `None`   | Describe the task the agent should achieve.                                                                                                                             |
| `instructions`                  | `List[str]` | `None`   | List of instructions added to the system prompt in `<instructions>` tags. Default instructions are also created depending on values for `markdown`, `output_model` etc. |
| `additional_context`            | `str`       | `None`   | Additional context added to the end of the system message.                                                                                                              |
| `expected_output`               | `str`       | `None`   | Provide the expected output from the Agent. This is added to the end of the system message.                                                                             |
| `markdown`                      | `bool`      | `False`  | Add an instruction to format the output using markdown.                                                                                                                 |
| `add_datetime_to_instructions`  | `bool`      | `False`  | If True, add the current datetime to the prompt to give the agent a sense of time. This allows for relative times like "tomorrow" to be used in the prompt              |
| `system_message`                | `str`       | `None`   | System prompt: provide the system prompt as a string                                                                                                                    |
| `system_message_role`           | `str`       | `system` | Role for the system message.                                                                                                                                            |
| `create_default_system_message` | `bool`      | `True`   | If True, build a default system prompt using agent settings and use that.                                                                                               |

<Tip>
  Disable the default system message by setting `create_default_system_message=False`.
</Tip>

## Default user message

The Agent creates a default user message, which is either the input message or a message with the `context` if `enable_rag=True`. The default user message can be customized using:

| Parameter                     | Type                      | Default  | Description                                                                                                                  |
| ----------------------------- | ------------------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `context`                     | `str`                     | `None`   | Additional context added to the end of the user message.                                                                     |
| `add_context`                 | `bool`                    | `False`  | If True, add the context to the user prompt.                                                                                 |
| `resolve_context`             | `bool`                    | `True`   | If True, resolve the context (i.e. call any functions in the context) before adding it to the user prompt.                   |
| `add_references`              | `bool`                    | `False`  | Enable RAG by adding references from the knowledge base to the prompt.                                                       |
| `retriever`                   | `Callable`                | `None`   | Function to get references to add to the user\_message. This function, if provided, is called when `add_references` is True. |
| `references_format`           | `Literal["json", "yaml"]` | `"json"` | Format of the references.                                                                                                    |
| `add_history_to_messages`     | `bool`                    | `False`  | If true, adds the chat history to the messages sent to the Model.                                                            |
| `num_history_responses`       | `int`                     | `3`      | Number of historical responses to add to the messages.                                                                       |
| `user_message`                | `Union[List, Dict, str]`  | `None`   | Provide the user prompt as a string. Note: this will ignore the message sent to the run function.                            |
| `user_message_role`           | `str`                     | `user`   | Role for the user message.                                                                                                   |
| `create_default_user_message` | `bool`                    | `True`   | If True, build a default user prompt using references and chat history.                                                      |

<Tip>
  Disable the default user message by setting `create_default_user_message=False`.
</Tip>


# Running your Agent
Source: https://docs.agno.com/agents/run

Learn how to run an agent and get the response.

The `Agent.run()` function runs the agent and generates a response, either as a `RunResponse` object or a stream of `RunResponse` objects.

Many of our examples use `agent.print_response()` which is a helper utility to print the response in the terminal. It uses `agent.run()` under the hood.

## Running your Agent

Here's how to run your agent. The response is captured in the `response`.

```python
from typing import Iterator
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

# Run agent and return the response as a variable
response: RunResponse = agent.run("Tell me a 5 second short story about a robot")

# Print the response in markdown format
pprint_run_response(response, markdown=True)
```

## RunResponse

The `Agent.run()` function returns a `RunResponse` object when not streaming. It has the following attributes:

<Note>
  Understanding Metrics

  For a detailed explanation of how metrics are collected and used, please refer to the [Metrics Documentation](/agents/metrics).
</Note>

See detailed documentation in the [RunResponse](/reference/agents/run-response) documentation.

## Streaming Responses

To enable streaming, set `stream=True` when calling `run()`. This will return an iterator of `RunResponseEvent` objects instead of a single response.

<Note>
  From `agno` version `1.6.0`, the `Agent.run()` function returns an iterator of `RunResponseEvent`, not of `RunResponse` objects.
</Note>

```python
from typing import Iterator
from agno.agent import Agent, RunResponseEvent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

agent = Agent(model=OpenAIChat(id="gpt-4-mini"))

# Run agent and return the response as a stream
response_stream: Iterator[RunResponseEvent] = agent.run(
    "Tell me a 5 second short story about a lion",
    stream=True
)

# Print the response stream in markdown format
pprint_run_response(response_stream, markdown=True)
```

### Streaming Intermediate Steps

For even more detailed streaming, you can enable intermediate steps by setting `stream_intermediate_steps=True`. This will provide real-time updates about the agent's internal processes.

```python
# Stream with intermediate steps
response_stream: Iterator[RunResponseEvent] = agent.run(
    "Tell me a 5 second short story about a lion",
    stream=True,
    stream_intermediate_steps=True
)
```

### Handling Events

You can process events as they arrive by iterating over the response stream:

```python
response_stream = agent.run("Your prompt", stream=True, stream_intermediate_steps=True)

for event in response_stream:
    if event.event == "RunResponseContent":
        print(f"Content: {event.content}")
    elif event.event == "ToolCallStarted":
        print(f"Tool call started: {event.tool}")
    elif event.event == "ReasoningStep":
        print(f"Reasoning step: {event.content}")
    ...
```

You can see this behavior in action in our [Playground](https://app.agno.com/playground/agents?endpoint=demo.agnoagents.com\&agent=reasoning-agent).

### Storing Events

You can store all the events that happened during a run on the `RunResponse` object.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

agent = Agent(model=OpenAIChat(id="gpt-4o-mini"), store_events=True)

response = agent.run("Tell me a 5 second short story about a lion", stream=True, stream_intermediate_steps=True)
pprint_run_response(response)

for event in agent.run_response.events:
    print(event.event)
```

By default the `RunResponseContentEvent` event is not stored. You can modify which events are skipped by setting the `events_to_skip` parameter.

For example:

```python
agent = Agent(model=OpenAIChat(id="gpt-4o-mini"), store_events=True, events_to_skip=[RunEvent.run_started.value])
```

### Event Types

The following events are yielded by the `Agent.run()` and `Agent.arun()` functions depending on the agent's configuration:

#### Core Events

| Event Type           | Description                                             |
| -------------------- | ------------------------------------------------------- |
| `RunStarted`         | Indicates the start of a run                            |
| `RunResponseContent` | Contains the model's response text as individual chunks |
| `RunCompleted`       | Signals successful completion of the run                |
| `RunError`           | Indicates an error occurred during the run              |
| `RunCancelled`       | Signals that the run was cancelled                      |

#### Control Flow Events

| Event Type     | Description                                  |
| -------------- | -------------------------------------------- |
| `RunPaused`    | Indicates the run has been paused            |
| `RunContinued` | Signals that a paused run has been continued |

#### Tool Events

| Event Type          | Description                                                    |
| ------------------- | -------------------------------------------------------------- |
| `ToolCallStarted`   | Indicates the start of a tool call                             |
| `ToolCallCompleted` | Signals completion of a tool call, including tool call results |

#### Reasoning Events

| Event Type           | Description                                          |
| -------------------- | ---------------------------------------------------- |
| `ReasoningStarted`   | Indicates the start of the agent's reasoning process |
| `ReasoningStep`      | Contains a single step in the reasoning process      |
| `ReasoningCompleted` | Signals completion of the reasoning process          |

#### Memory Events

| Event Type              | Description                                     |
| ----------------------- | ----------------------------------------------- |
| `MemoryUpdateStarted`   | Indicates that the agent is updating its memory |
| `MemoryUpdateCompleted` | Signals completion of a memory update           |

See detailed documentation in the [RunResponseEvent](/reference/agents/run-response) documentation.

## Structured Input

An agent can be provided with structured input (i.e a pydantic model) by passing it in the `Agent.run()` or `Agent.print_response()` as the `message` parameter.

```python
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field


class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements"""

    topic: str
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)


# Define agents
hackernews_agent = Agent(
    name="Hackernews Agent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[HackerNewsTools()],
    role="Extract key insights and content from Hackernews posts",
)

hackernews_agent.print_response(
    message=ResearchTopic(
        topic="AI",
        focus_areas=["AI", "Machine Learning"],
        target_audience="Developers",
        sources_required=5,
    )
)
```


# Sessions
Source: https://docs.agno.com/agents/sessions



When we call `Agent.run()`, it creates a stateless, singular Agent run.

But what if we want to continue this run i.e. have a multi-turn conversation? That's where `sessions` come in. A session is collection of consecutive runs.

In practice, a session is a multi-turn conversation between a user and an Agent. Using a `session_id`, we can connect the conversation history and state across multiple runs.

Let's outline some key concepts:

* **User:** A user represents an individual that interacts with the Agent. Each user has associated memories, sessions, and conversation history separate from other users.
* **Session:** A session is collection of consecutive runs like a multi-turn conversation between a user and an Agent. Sessions are identified by a `session_id` and each turn is a **run**.
* **Run:** Every interaction (i.e. chat or turn) with an Agent is called a **run**. Runs are identified by a `run_id` and `Agent.run()` creates a new `run_id` when called.
* **Messages:** are the individual messages sent between the model and the Agent. Messages are the communication protocol between the Agent and model.

Let's start with an example where a single run is created with an Agent. A `run_id` is automatically generated, as well as a `session_id` (because we didn't provide one to continue the conversation). This run is not yet associated with a user.

```python
from typing import Iterator
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

agent = Agent(model=OpenAIChat(id="gpt-4o-mini"))

# Run agent and return the response as a variable
agent.print_response("Tell me a 5 second short story about a robot")
```

## Multi-user, multi-session Agents

Each user that is interacting with an Agent gets a unique set of sessions and you can have multiple users interacting with the same Agent at the same time.

Set a `user_id` to connect a user to their sessions with the Agent.

In the example below, we set a `session_id` to demo how to have multi-turn conversations with multiple users at the same time. In production, the `session_id` is auto generated.

<Note>
  Note: Multi-user, multi-session currently only works with `Memory.v2`, which will become the default memory implementation in the next release.
</Note>

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.memory.v2 import Memory

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Multi-user, multi-session only work with Memory.v2
    memory=Memory(),
    add_history_to_messages=True,
    num_history_runs=3,
)

user_1_id = "user_101"
user_2_id = "user_102"

user_1_session_id = "session_101"
user_2_session_id = "session_102"

# Start the session with user 1
agent.print_response(
    "Tell me a 5 second short story about a robot.",
    user_id=user_1_id,
    session_id=user_1_session_id,
)
# Continue the session with user 1
agent.print_response("Now tell me a joke.", user_id=user_1_id, session_id=user_1_session_id)

# Start the session with user 2
agent.print_response("Tell me about quantum physics.", user_id=user_2_id, session_id=user_2_session_id)
# Continue the session with user 2
agent.print_response("What is the speed of light?", user_id=user_2_id, session_id=user_2_session_id)

# Ask the agent to give a summary of the conversation, this will use the history from the previous messages
agent.print_response(
    "Give me a summary of our conversation.",
    user_id=user_1_id,
    session_id=user_1_session_id,
)
```

## Fetch messages from last N sessions

In some scenarios, you might want to fetch messages from the last N sessions to provide context or continuity in conversations.

Here's an example of how you can achieve this:

```python
# Remove the tmp db file before running the script
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage

os.remove("tmp/data.db")

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    user_id="user_1",
    storage=SqliteStorage(table_name="agent_sessions_new", db_file="tmp/data.db"),
    search_previous_sessions_history=True,  # allow searching previous sessions
    num_history_sessions=2,  # only include the last 2 sessions in the search to avoid context length issues
    show_tool_calls=True,
)

session_1_id = "session_1_id"
session_2_id = "session_2_id"
session_3_id = "session_3_id"
session_4_id = "session_4_id"
session_5_id = "session_5_id"

agent.print_response("What is the capital of South Africa?", session_id=session_1_id)
agent.print_response("What is the capital of China?", session_id=session_2_id)
agent.print_response("What is the capital of France?", session_id=session_3_id)
agent.print_response("What is the capital of Japan?", session_id=session_4_id)
agent.print_response(
    "What did I discuss in my previous conversations?", session_id=session_5_id
)  # It should only include the last 2 sessions
```

<Note>
  To enable fetching messages from the last N sessions, you need to use the following flags:

  * `search_previous_sessions_history`: Set this to `True` to allow searching through previous sessions.
  * `num_history_sessions`: Specify the number of past sessions to include in the search. In this example, it is set to `2` to include only the last 2 sessions.
    It's advisable to keep this number to 2 or 3 for now, as a larger number might fill up the context length of the model, potentially leading to performance issues.

  These flags help manage the context length and ensure that only relevant session history is included in the conversation.
</Note>


# Agent State
Source: https://docs.agno.com/agents/state



**State** is any kind of data the Agent needs to maintain throughout runs.

<Check>
  A simple yet common use case for Agents is to manage lists, items and other "information" for a user. For example, a shopping list, a todo list, a wishlist, etc.

  This can be easily managed using the `session_state`. The Agent updates the `session_state` in tool calls and exposes them to the Model in the `description` and `instructions`.
</Check>

Agno's provides a powerful and elegant state management system, here's how it works:

* The `Agent` has a `session_state` parameter.
* We add our state variables to this `session_state` dictionary.
* We update the `session_state` dictionary in tool calls or other functions.
* We share the current `session_state` with the Model in the `description` and `instructions`.
* The `session_state` is stored with Agent sessions and is persisted in a database. Meaning, it is available across execution cycles. This also means when switching sessions between calls to `agent.run()`, the state is loaded and available.
* You can also pass `session_state` to the agent on `agent.run()`, effectively overriding any state that was set on Agent initialization.

Here's an example of an Agent managing a shopping list:

```python session_state.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat

# Define a tool that adds an item to the shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    agent.session_state["shopping_list"].append(item)
    return f"The shopping list is now {agent.session_state['shopping_list']}"


# Create an Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a counter starting at 0
    session_state={"shopping_list": []},
    tools=[add_item],
    # You can use variables from the session state in the instructions
    instructions="Current state (shopping list) is: {shopping_list}",
    # Important: Add the state to the messages
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response("Add milk, eggs, and bread to the shopping list", stream=True)
print(f"Final session state: {agent.session_state}")
```

<Tip>
  This is as good and elegant as state management gets.
</Tip>

## Maintaining state across multiple runs

A big advantage of **sessions** is the ability to maintain state across multiple runs. For example, let's say the agent is helping a user keep track of their shopping list.

<Note>
  By setting `add_state_in_messages=True`, the keys of the `session_state` dictionary are available in the `description` and `instructions` as variables.

  Use this pattern to add the shopping\_list to the instructions directly.
</Note>

```python shopping_list.py
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat


# Define tools to manage our shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list and return confirmation."""
    # Add the item if it's not already in the list
    if item.lower() not in [i.lower() for i in agent.session_state["shopping_list"]]:
        agent.session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"


def remove_item(agent: Agent, item: str) -> str:
    """Remove an item from the shopping list by name."""
    # Case-insensitive search
    for i, list_item in enumerate(agent.session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            agent.session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

    return f"'{item}' was not found in the shopping list"


def list_items(agent: Agent) -> str:
    """List all items in the shopping list."""
    shopping_list = agent.session_state["shopping_list"]

    if not shopping_list:
        return "The shopping list is empty."

    items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"


# Create a Shopping List Manager Agent that maintains state
agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with an empty shopping list
    session_state={"shopping_list": []},
    tools=[add_item, remove_item, list_items],
    # You can use variables from the session state in the instructions
    instructions=dedent("""\
        Your job is to manage a shopping list.

        The shopping list starts empty. You can add items, remove items by name, and list all items.

        Current shopping list: {shopping_list}
    """),
    show_tool_calls=True,
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response("Add milk, eggs, and bread to the shopping list", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("I got bread", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("I need apples and oranges", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("whats on my list?", stream=True)
print(f"Session state: {agent.session_state}")

agent.print_response("Clear everything from my list and start over with just bananas and yogurt", stream=True)
print(f"Session state: {agent.session_state}")
```

<Tip>
  State is a great way to control context across multiple runs.
</Tip>

## Using state in instructions

You can use variables from the session state in the instructions by setting `add_state_in_messages=True`.

<Tip>
  Don't use the f-string syntax in the instructions. Directly use the `{key}` syntax, Agno substitutes the values for you.
</Tip>

```python state_in_instructions.py
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Initialize the session state with a variable
    session_state={"user_name": "John"},
    # You can use variables from the session state in the instructions
    instructions="Users name is {user_name}",
    show_tool_calls=True,
    add_state_in_messages=True,
    markdown=True,
)

agent.print_response("What is my name?", stream=True)
```

## Changing state on run

When you pass `session_id` to the agent on `agent.run()`, it will switch to the session with the given `session_id` and load any state that was set on that session.

This is useful when you want to continue a session for a specific user.

```python changing_state_on_run.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    add_state_in_messages=True,
    instructions="Users name is {user_name} and age is {age}",
)

# Sets the session state for the session with the id "user_1_session_1"
agent.print_response("What is my name?", session_id="user_1_session_1", user_id="user_1", session_state={"user_name": "John", "age": 30})

# Will load the session state from the session with the id "user_1_session_1"
agent.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

# Sets the session state for the session with the id "user_2_session_1"
agent.print_response("What is my name?", session_id="user_2_session_1", user_id="user_2", session_state={"user_name": "Jane", "age": 25})

# Will load the session state from the session with the id "user_2_session_1"
agent.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")
```

## Persisting state in database

`session_state` is part of the Agent session and is saved to the database after each run if a `storage` driver is provided.

Here's an example of an Agent that maintains a shopping list and persists the state in a database. Run this script multiple times to see the state being persisted.

```python session_state_storage.py
"""Run `pip install agno openai sqlalchemy` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage


# Define a tool that adds an item to the shopping list
def add_item(agent: Agent, item: str) -> str:
    """Add an item to the shopping list."""
    if item not in agent.session_state["shopping_list"]:
        agent.session_state["shopping_list"].append(item)
    return f"The shopping list is now {agent.session_state['shopping_list']}"


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Fix the session id to continue the same session across execution cycles
    session_id="fixed_id_for_demo",
    # Initialize the session state with an empty shopping list
    session_state={"shopping_list": []},
    # Add a tool that adds an item to the shopping list
    tools=[add_item],
    # Store the session state in a SQLite database
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/data.db"),
    # Add the current shopping list from the state in the instructions
    instructions="Current shopping list is: {shopping_list}",
    # Important: Set `add_state_in_messages=True`
    # to make `{shopping_list}` available in the instructions
    add_state_in_messages=True,
    markdown=True,
)

# Example usage
agent.print_response("What's on my shopping list?", stream=True)
print(f"Session state: {agent.session_state}")
agent.print_response("Add milk, eggs, and bread", stream=True)
print(f"Session state: {agent.session_state}")
```


# Session Storage
Source: https://docs.agno.com/agents/storage



Use **Session Storage** to persist Agent sessions and state to a database or file.

<Tip>
  **Why do we need Session Storage?**

  Agents are ephemeral and the built-in memory only lasts for the current execution cycle.

  In production environments, we serve (or trigger) Agents via an API and need to continue the same session across multiple requests. Storage persists the session history and state in a database and allows us to pick up where we left off.

  Storage also let's us inspect and evaluate Agent sessions, extract few-shot examples and build internal monitoring tools. It lets us **look at the data** which helps us build better Agents.
</Tip>

Adding storage to an Agent, Team or Workflow is as simple as providing a `Storage` driver and Agno handles the rest. You can use Sqlite, Postgres, Mongo or any other database you want.

Here's a simple example that demostrates persistence across execution cycles:

```python storage.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.storage.sqlite import SqliteStorage
from rich.pretty import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    # Fix the session id to continue the same session across execution cycles
    session_id="fixed_id_for_demo",
    storage=SqliteStorage(table_name="agent_sessions", db_file="tmp/data.db"),
    add_history_to_messages=True,
    num_history_runs=3,
)
agent.print_response("What was my last question?")
agent.print_response("What is the capital of France?")
agent.print_response("What was my last question?")
pprint(agent.get_messages_for_session())
```

The first time you run this, the answer to "What was my last question?" will not be available. But run it again and the Agent will able to answer properly. Because we have fixed the session id, the Agent will continue from the same session every time you run the script.

## Benefits of Storage

Storage has typically been an under-discussed part of Agent Engineering -- but we see it as the unsung hero of production agentic applications.

In production, you need storage to:

* Continue sessions: retrieve sessions history and pick up where you left off.
* Get list of sessions: To continue a previous session, you need to maintain a list of sessions available for that agent.
* Save state between runs: save the Agent's state to a database or file so you can inspect it later.

But there is so much more:

* Storage saves our Agent's session data for inspection and evaluations.
* Storage helps us extract few-shot examples, which can be used to improve the Agent.
* Storage enables us to build internal monitoring tools and dashboards.

<Warning>
  Storage is such a critical part of your Agentic infrastructure that it should never be offloaded to a third party. You should almost always use your own storage layer for your Agents.
</Warning>

## Example: Use Postgres for storage

<Steps>
  <Step title="Run Postgres">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Postgres** on port **5532** using:

    ```bash
    docker run -d \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -e PGDATA=/var/lib/postgresql/data/pgdata \
      -v pgvolume:/var/lib/postgresql/data \
      -p 5532:5432 \
      --name pgvector \
      agno/pgvector:16
    ```
  </Step>

  <Step title="Create an Agent with Storage">
    Create a file `agent_with_storage.py` with the following contents

    ```python
    import typer
    from typing import Optional, List
    from agno.agent import Agent
    from agno.storage.postgres import PostgresStorage
    from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
    from agno.vectordb.pgvector import PgVector, SearchType

    db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=PgVector(table_name="recipes", db_url=db_url, search_type=SearchType.hybrid),
    )
    storage = PostgresStorage(table_name="pdf_agent", db_url=db_url)

    def pdf_agent(new: bool = False, user: str = "user"):
        session_id: Optional[str] = None

        if not new:
            existing_sessions: List[str] = storage.get_all_session_ids(user)
            if len(existing_sessions) > 0:
                session_id = existing_sessions[0]

        agent = Agent(
            session_id=session_id,
            user_id=user,
            knowledge=knowledge_base,
            storage=storage,
            # Show tool calls in the response
            show_tool_calls=True,
            # Enable the agent to read the chat history
            read_chat_history=True,
            # We can also automatically add the chat history to the messages sent to the model
            # But giving the model the chat history is not always useful, so we give it a tool instead
            # to only use when needed.
            # add_history_to_messages=True,
            # Number of historical responses to add to the messages.
            # num_history_responses=3,
        )
        if session_id is None:
            session_id = agent.session_id
            print(f"Started Session: {session_id}\n")
        else:
            print(f"Continuing Session: {session_id}\n")

        # Runs the agent as a cli app
        agent.cli_app(markdown=True)


    if __name__ == "__main__":
        # Load the knowledge base: Comment after first run
        knowledge_base.load(upsert=True)

        typer.run(pdf_agent)
    ```
  </Step>

  <Step title="Run the agent">
    Install libraries

    <CodeGroup>
      ```bash Mac
      pip install -U agno openai pgvector pypdf "psycopg[binary]" sqlalchemy
      ```

      ```bash Windows
      pip install -U agno openai pgvector pypdf "psycopg[binary]" sqlalchemy
      ```
    </CodeGroup>

    Run the agent

    ```bash
    python agent_with_storage.py
    ```

    Now the agent continues across sessions. Ask a question:

    ```
    How do I make pad thai?
    ```

    Then message `bye` to exit, start the app again and ask:

    ```
    What was my last message?
    ```
  </Step>

  <Step title="Start a new run">
    Run the `agent_with_storage.py` file with the `--new` flag to start a new run.

    ```bash
    python agent_with_storage.py --new
    ```
  </Step>
</Steps>

## Schema Upgrades

When using `AgentStorage`, the SQL-based storage classes have fixed schemas. As new Agno features are released, the schemas might need to be updated.

Upgrades can either be done manually or automatically.

### Automatic Upgrades

Automatic upgrades are done when the `auto_upgrade_schema` parameter is set to `True` in the storage class constructor.
You only need to set this once for an agent run and the schema would be upgraded.

```python
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
storage = PostgresStorage(table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True)
```

### Manual Upgrades

Manual schema upgrades can be done by calling the `upgrade_schema` method on the storage class.

```python
db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"
storage = PostgresStorage(table_name="agent_sessions", db_url=db_url)
storage.upgrade_schema()
```

## Params

| Parameter | Type                     | Default | Description                      |
| --------- | ------------------------ | ------- | -------------------------------- |
| `storage` | `Optional[AgentStorage]` | `None`  | Storage mechanism for the agent. |

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/storage)


# Structured Output
Source: https://docs.agno.com/agents/structured-output



One of our favorite features is using Agents to generate structured data (i.e. a pydantic model). Use this feature to extract features, classify data, produce fake data etc. The best part is that they work with function calls, knowledge bases and all other features.

## Example

Let's create an Movie Agent to write a `MovieScript` for us.

```python movie_agent.py
from typing import List
from rich.pretty import pprint
from pydantic import BaseModel, Field
from agno.agent import Agent, RunResponse
from agno.models.openai import OpenAIChat

class MovieScript(BaseModel):
    setting: str = Field(..., description="Provide a nice setting for a blockbuster movie.")
    ending: str = Field(..., description="Ending of the movie. If not available, provide a happy ending.")
    genre: str = Field(
        ..., description="Genre of the movie. If not available, select action, thriller or romantic comedy."
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(..., description="3 sentence storyline for the movie. Make it exciting!")

# Agent that uses JSON mode
json_mode_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
    use_json_mode=True,
)
json_mode_agent.print_response("New York")

# Agent that uses structured outputs
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

structured_output_agent.print_response("New York")
```

Run the script to see the output.

```bash
pip install -U agno openai

python movie_agent.py
```

The output is an object of the `MovieScript` class, here's how it looks:

```python
# Using JSON mode
MovieScript(
│   setting='The bustling streets of New York City, filled with skyscrapers, secret alleyways, and hidden underground passages.',
│   ending='The protagonist manages to thwart an international conspiracy, clearing his name and winning the love of his life back.',
│   genre='Thriller',
│   name='Shadows in the City',
│   characters=['Alex Monroe', 'Eva Parker', 'Detective Rodriguez', 'Mysterious Mr. Black'],
│   storyline="When Alex Monroe, an ex-CIA operative, is framed for a crime he didn't commit, he must navigate the dangerous streets of New York to clear his name. As he uncovers a labyrinth of deceit involving the city's most notorious crime syndicate, he enlists the help of an old flame, Eva Parker. Together, they race against time to expose the true villain before it's too late."
)

# Use the structured output
MovieScript(
│   setting='In the bustling streets and iconic skyline of New York City.',
│   ending='Isabella and Alex, having narrowly escaped the clutches of the Syndicate, find themselves standing at the top of the Empire State Building. As the glow of the setting sun bathes the city, they share a victorious kiss. Newly emboldened and as an unstoppable duo, they vow to keep NYC safe from any future threats.',
│   genre='Action Thriller',
│   name='The NYC Chronicles',
│   characters=['Isabella Grant', 'Alex Chen', 'Marcus Kane', 'Detective Ellie Monroe', 'Victor Sinclair'],
│   storyline='Isabella Grant, a fearless investigative journalist, uncovers a massive conspiracy involving a powerful syndicate plotting to control New York City. Teaming up with renegade cop Alex Chen, they must race against time to expose the culprits before the city descends into chaos. Dodging danger at every turn, they fight to protect the city they love from imminent destruction.'
)
```

## Using a Parser Model

You can use an additional model to parse and structure the output from your primary model. A `parser_model` can be employed to transform the raw response into structured output, replacing the default model for this specific task. This approach is particularly effective when the primary model is optimized for reasoning tasks, as such models may not consistently produce detailed structured responses.

```python
agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    description="You write movie scripts.",
    response_model=MovieScript,
    parser_model=OpenAIChat(id="gpt-4o"),
)
```

You can also provide a custom `parser_model_prompt` to your Parser Model.

## Streaming Structured Output

Streaming can be used in combination with `response_model`. This returns the structured output as a single event in the stream of events.

```python streaming_agent.py
import asyncio
from typing import Dict, List

from agno.agent import Agent
from agno.models.openai.chat import OpenAIChat
from pydantic import BaseModel, Field


class MovieScript(BaseModel):
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    name: str = Field(..., description="Give a name to this movie")
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )
    rating: Dict[str, int] = Field(
        ...,
        description="Your own rating of the movie. 1-10. Return a dictionary with the keys 'story' and 'acting'.",
    )


# Agent that uses structured outputs with streaming
structured_output_agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    description="You write movie scripts.",
    response_model=MovieScript,
)

structured_output_agent.print_response(
    "New York", stream=True, stream_intermediate_steps=True
)
```

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/async/structured_output.py)
* View [Parser Model Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/other/parse_model.py)
* View [Streaming Structured Output](https://github.com/agno-agi/agno/blob/main/cookbook/models/openai/chat/structured_output_stream.py)


# Agent Teams [Deprecated]
Source: https://docs.agno.com/agents/teams



<Note>
  Agent Teams were an initial implementation of our multi-agent architecture (2023-2025) that use a transfer/handoff mechanism. After 2 years of experimentation, we've learned that this mechanism is not scalable and do NOT recommend it for complex multi-agent systems.

  Please use the new [Teams](/teams) architecture instead.
</Note>

We can combine multiple Agents to form a team and tackle tasks as a cohesive unit. Here's a simple example that converts an agent into a team to write an article about the top stories on hackernews.

```python hackernews_team.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.hackernews import HackerNewsTools
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-4o"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-4o"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_instructions=True,
)

article_reader = Agent(
    name="Article Reader",
    model=OpenAIChat("gpt-4o"),
    role="Reads articles from URLs.",
    tools=[Newspaper4kTools()],
)

hn_team = Agent(
    name="Hackernews Team",
    model=OpenAIChat("gpt-4o"),
    team=[hn_researcher, web_searcher, article_reader],
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the article reader to read the links for the stories to get more information.",
        "Important: you must provide the article reader with the links to read.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    show_tool_calls=True,
    markdown=True,
)
hn_team.print_response("Write an article about the top 2 stories on hackernews", stream=True)
```

Run the script to see the output.

```bash
pip install -U openai duckduckgo-search newspaper4k lxml_html_clean agno

python hackernews_team.py
```

## How to build Agent Teams

1. Add a `name` and `role` parameter to the member Agents.
2. Create a Team Leader that can delegate tasks to team-members.
3. Use your Agent team just like you would use a regular Agent.


# Tools
Source: https://docs.agno.com/agents/tools

Learn how to use tools in Agno to build AI agents.

**Agents use tools to take actions and interact with external systems**.

Tools are functions that an Agent can run to achieve tasks. For example: searching the web, running SQL, sending an email or calling APIs. You can use any python function as a tool or use a pre-built **toolkit**. The general syntax is:

```python
from agno.agent import Agent

agent = Agent(
    # Add functions or Toolkits
    tools=[...],
    # Show tool calls in the Agent response
    show_tool_calls=True
)
```

## Using a Toolkit

Agno provides many pre-built **toolkits** that you can add to your Agents. For example, let's use the DuckDuckGo toolkit to search the web.

<Tip>You can find more toolkits in the [Toolkits](/tools/toolkits) guide.</Tip>

<Steps>
  <Step title="Create Web Search Agent">
    Create a file `web_search.py`

    ```python web_search.py
    from agno.agent import Agent
    from agno.tools.duckduckgo import DuckDuckGoTools

    agent = Agent(tools=[DuckDuckGoTools()], show_tool_calls=True, markdown=True)
    agent.print_response("Whats happening in France?", stream=True)
    ```
  </Step>

  <Step title="Run the agent">
    Install libraries

    ```shell
    pip install openai duckduckgo-search agno
    ```

    Run the agent

    ```shell
    python web_search.py
    ```
  </Step>
</Steps>

## Writing your own Tools

For more control, write your own python functions and add them as tools to an Agent. For example, here's how to add a `get_top_hackernews_stories` tool to an Agent.

```python hn_agent.py
import json
import httpx

from agno.agent import Agent

def get_top_hackernews_stories(num_stories: int = 10) -> str:
    """Use this function to get top stories from Hacker News.

    Args:
        num_stories (int): Number of stories to return. Defaults to 10.

    Returns:
        str: JSON string of top stories.
    """

    # Fetch top story IDs
    response = httpx.get('https://hacker-news.firebaseio.com/v0/topstories.json')
    story_ids = response.json()

    # Fetch story details
    stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json')
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        stories.append(story)
    return json.dumps(stories)

agent = Agent(tools=[get_top_hackernews_stories], show_tool_calls=True, markdown=True)
agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)
```

Read more about:

* [Available toolkits](/tools/toolkits)
* [Using functions as tools](/tools/tool-decorator)

## Attributes

The following attributes allow an `Agent` to use tools

| Parameter                | Type                                                   | Default | Description                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ------------------------ | ------------------------------------------------------ | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `tools`                  | `List[Union[Tool, Toolkit, Callable, Dict, Function]]` | -       | A list of tools provided to the Model. Tools are functions the model may generate JSON inputs for.                                                                                                                                                                                                                                                                                                                                                  |
| `show_tool_calls`        | `bool`                                                 | `False` | Print the signature of the tool calls in the Model response.                                                                                                                                                                                                                                                                                                                                                                                        |
| `tool_call_limit`        | `int`                                                  | -       | Maximum number of tool calls allowed for a single run.                                                                                                                                                                                                                                                                                                                                                                                              |
| `tool_choice`            | `Union[str, Dict[str, Any]]`                           | -       | Controls which (if any) tool is called by the model. "none" means the model will not call a tool and instead generates a message. "auto" means the model can pick between generating a message or calling a tool. Specifying a particular function via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool. "none" is the default when no tools are present. "auto" is the default if tools are present. |
| `read_chat_history`      | `bool`                                                 | `False` | Add a tool that allows the Model to read the chat history.                                                                                                                                                                                                                                                                                                                                                                                          |
| `search_knowledge`       | `bool`                                                 | `False` | Add a tool that allows the Model to search the knowledge base (aka Agentic RAG).                                                                                                                                                                                                                                                                                                                                                                    |
| `update_knowledge`       | `bool`                                                 | `False` | Add a tool that allows the Model to update the knowledge base.                                                                                                                                                                                                                                                                                                                                                                                      |
| `read_tool_call_history` | `bool`                                                 | `False` | Add a tool that allows the Model to get the tool call history.                                                                                                                                                                                                                                                                                                                                                                                      |

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools)


# User Control Flows
Source: https://docs.agno.com/agents/user-control-flow

Learn how to control the flow of an agent's execution in Agno. This is also called "Human in the Loop".

User control flows in Agno enable you to implement "Human in the Loop" patterns, where human oversight and input are required during agent execution. This is crucial for:

* Validating sensitive operations
* Reviewing tool calls before execution
* Gathering user input for decision-making
* Managing external tool execution

## Types of User Control Flows

Agno supports four main types of user control flows:

1. **User Confirmation**: Require explicit user approval before executing tool calls
2. **User Input**: Gather specific information from users during execution
3. **Dynamic User Input**: Have the agent collect user input as it needs it
4. **External Tool Execution**: Execute tools outside of the agent's control

## Pausing Agent Execution

User control flows interrupt the agent's execution and require human oversight. The run can then be continued by calling the `continue_run` method.

For example:

```python
agent.run("Perform sensitive operation")

if agent.is_paused:
    # The agent will pause while human input is provided
    # ... perform other tasks

    # The user can then continue the run
    response = agent.continue_run()
    # or response = await agent.acontinue_run()
```

The `continue_run` method continues with the state of the agent at the time of the pause.  You can also pass the `run_response` of a specific run to the `continue_run` method, or the `run_id`.

## User Confirmation

User confirmation allows you to pause execution and require explicit user approval before proceeding with tool calls. This is useful for:

* Sensitive operations
* API calls that modify data
* Actions with significant consequences

The following example shows how to implement user confirmation.

```python
from agno.tools import tool
from agno.agent import Agent
from agno.models.openai import OpenAIChat

@tool(requires_confirmation=True)
def sensitive_operation(data: str) -> str:
    """Perform a sensitive operation that requires confirmation."""
    # Implementation here
    return "Operation completed"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[sensitive_operation],
)

# Run the agent
agent.run("Perform sensitive operation")

# Handle confirmation
if agent.is_paused:
    for tool in agent.run_response.tools_requiring_confirmation:
        # Get user confirmation
        print(f"Tool {tool.tool_name}({tool.tool_args}) requires confirmation")
        confirmed = input(f"Confirm? (y/n): ").lower() == "y"
        tool.confirmed = confirmed

  # Continue execution
  response = agent.continue_run()
```

You can also specify which tools in a toolkit require confirmation.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.yfinance import YFinanceTools
from agno.utils import pprint

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools(requires_confirmation_tools=["get_current_stock_price"])],
)

agent.run("What is the current stock price of Apple?")
if agent.is_paused:
    for tool in agent.run_response.tools_requiring_confirmation:
        print(f"Tool {tool.tool_name}({tool.tool_args}) requires confirmation")
        confirmed = input(f"Confirm? (y/n): ").lower() == "y"

        if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

    run_response = agent.continue_run()
    pprint.pprint_run_response(run_response)
```

## User Input

User input flows allow you to gather specific information from users during execution. This is useful for:

* Collecting required parameters
* Getting user preferences
* Gathering missing information

In the example below, we require all the input for the `send_email` tool from the user.

```python
from typing import List
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.function import UserInputField

# We still provide a docstring to the tool; This will be used to populate the `user_input_schema`
@tool(requires_user_input=True)
def send_email(to: str, subject: str, body: str) -> dict:
    """Send an email to the user.

    Args:
        to (str): The address to send the email to.
        subject (str): The subject of the email.
        body (str): The body of the email.
    """
    # Implementation here
    return f"Email sent to {to} with subject {subject} and body {body}"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[send_email],
)

agent.run("Send an email please")
if agent.is_paused:
    for tool in agent.run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Display field information to the user
            print(f"\nField: {field.name} ({field.field_type.__name__}) -> {field.description}")

            # Get user input
            user_value = input(f"Please enter a value for {field.name}: ")

            # Update the field value
            field.value = user_value

    run_response = (
        agent.continue_run()
    )
```

The `RunResponse` object has a list of tools and in the case of `requires_user_input`, the tools that require input will have `user_input_schema` populated.
This is a list of `UserInputField` objects.

```python
class UserInputField:
    name: str  # The name of the field
    field_type: Type  # The required type of the field
    description: Optional[str] = None  # The description of the field
    value: Optional[Any] = None  # The value of the field. Populated by the agent or the user.
```

You can also specify which fields should be filled by the user while the agent will provide the rest of the fields.

```python

# You can either specify the user_input_fields leave empty for all fields to be provided by the user
@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

    Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"

agent = Agent(
    model=OpenAIChat(id="gpt-4o"),
    tools=[send_email],
)

agent.run("Send an email with the subject 'Hello' and the body 'Hello, world!'")
if agent.is_paused:
    for tool in agent.run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Display field information to the user
            print(f"\nField: {field.name} ({field.field_type.__name__}) -> {field.description}")

            # Get user input (if the value is not set, it means the user needs to provide the value)
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
                field.value = user_value
            else:
                print(f"Value provided by the agent: {field.value}")

    run_response = (
        agent.continue_run()
    )
```

## Dynamic User Input

This pattern provides the agent with tools to indicate when it needs user input. It's ideal for:

* Cases where it is unknown how the user will interact with the agent
* When you want a form-like interaction with the user

In the following example, we use a specialized tool to allow the agent to collect user feedback when it needs it.

```python
from typing import Any, Dict

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.toolkit import Toolkit
from agno.tools.user_control_flow import UserControlFlowTools
from agno.utils import pprint

# Example toolkit for handling emails
class EmailTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="EmailTools", tools=[self.send_email, self.get_emails], *args, **kwargs
        )

    def send_email(self, subject: str, body: str, to_address: str) -> str:
        """Send an email to the given address with the given subject and body.

        Args:
            subject (str): The subject of the email.
            body (str): The body of the email.
            to_address (str): The address to send the email to.
        """
        return f"Sent email to {to_address} with subject {subject} and body {body}"

    def get_emails(self, date_from: str, date_to: str) -> str:
        """Get all emails between the given dates.

        Args:
            date_from (str): The start date.
            date_to (str): The end date.
        """
        return [
            {
                "subject": "Hello",
                "body": "Hello, world!",
                "to_address": "test@test.com",
                "date": date_from,
            },
            {
                "subject": "Random other email",
                "body": "This is a random other email",
                "to_address": "john@doe.com",
                "date": date_to,
            },
        ]


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[EmailTools(), UserControlFlowTools()],
    markdown=True,
    debug_mode=True,
)

run_response = agent.run("Send an email with the body 'How is it going in Tokyo?'")

# We use a while loop to continue the running until the agent is satisfied with the user input
while run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

        for field in input_schema:
            # Display field information to the user
            print(f"\nField: {field.name} ({field.field_type.__name__}) -> {field.description}")

            # Get user input (if the value is not set, it means the user needs to provide the value)
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
                field.value = user_value
            else:
                print(f"Value provided by the agent: {field.value}")

    run_response = agent.continue_run(run_response=run_response)

    # If the agent is not paused for input, we are done
    if not run_response.is_paused:
        pprint.pprint_run_response(run_response)
        break
```

## External Tool Execution

External tool execution allows you to execute tools outside of the agent's control. This is useful for:

* External service calls
* Database operations

```python
import subprocess

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.utils import pprint


# We have to create a tool with the correct name, arguments and docstring for the agent to know what to call.
@tool(external_execution=True)
def execute_shell_command(command: str) -> str:
    """Execute a shell command.

    Args:
        command (str): The shell command to execute

    Returns:
        str: The output of the shell command
    """
    return subprocess.check_output(command, shell=True).decode("utf-8")


agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[execute_shell_command],
    markdown=True,
)

run_response = agent.run("What files do I have in my current directory?")
if run_response.is_paused:
    for tool in run_response.tools_awaiting_external_execution:
        if tool.tool_name == execute_shell_command.name:
            print(f"Executing {tool.tool_name} with args {tool.tool_args} externally")

            # We execute the tool ourselves. You can execute any function or process here and use the tool_args as input.
            result = execute_shell_command.entrypoint(**tool.tool_args)
            # We have to set the result on the tool execution object so that the agent can continue
            tool.result = result

    run_response = agent.continue_run()
    pprint.pprint_run_response(run_response)
```

## Best Practices

1. **Sanitise user input**: Always validate and sanitize user input to prevent security vulnerabilities.
2. **Error Handling**: Always implement proper error handling for user input and external calls
3. **Input Validation**: Validate user input before processing

<Note>
  If you are using streaming, it is recommended to pass the `run_id` and a list of `updated_tools` to the `continue_run` or `acontinue_run` method.
  See an example [here](/examples/concepts/user-control-flows/10-external-tool-execution-stream.mdx).
</Note>

## Developer Resources

* View more [Examples](/examples/concepts/user-control-flows)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agent_concepts/user_control_flows)


# AG-UI App
Source: https://docs.agno.com/applications/ag-ui/introduction

Expose your Agno Agent as a AG-UI compatible app

AG-UI, or [Agent-User Interaction Protocol](https://github.com/ag-ui-protocol/ag-ui), is a protocol standarizing how AI agents connect to front-end applications.

## Example usage

<Steps>
  <Step title="Install the backend dependencies">
    ```bash
    pip install agno ag-ui-protocol
    ```
  </Step>

  <Step title="Run the backend">
    Now let's run a `AGUIApp` exposing an Agno Agent. You can use the previous code!
  </Step>

  <Step title="Run the frontend">
    You can use [Dojo](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo), an advanced and customizable option to use as frontend for AG-UI agents.

    1. Clone the project: `git clone https://github.com/ag-ui-protocol/ag-ui.git`
    2. Follow the instructions [here](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo) to learn how to install the needed dependencies and run the project.
    3. Remember to install the dependencies in `/ag-ui/typescript-sdk` with `pnpm install`, and to build the Agno package in `/integrations/agno` with `pnpm run build`.
    4. You can now run your Dojo! It will show our Agno agent as one of the available options.
  </Step>

  <Step title="Chat with your Agno Agent">
    Done! If you are running Dojo as your front-end, you can now go to [http://localhost:3000](http://localhost:3000) in your browser and chat with your Agno Agent.
  </Step>
</Steps>

<img src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=348f581def22eee7b9429b08d73d3502" alt="AG-UI Dojo screenshot" width="2820" height="1706" data-path="images/agui-dojo.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?w=280&maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=4c0a37cc36358a28be906a8cdf5acf5f 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?w=560&maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=6e0415c9c76a26d8e3092f4f46a3b57c 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?w=840&maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=09a0e679b93ebbae76a3a78f14a88d9b 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?w=1100&maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=1c8c86ece0f20568b44dcfe7a9e98f65 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?w=1650&maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=ec7c926686e014738963fedbee2abbf1 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/agui-dojo.png?w=2500&maxW=2820&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=8b58a129da8c78c55ce4dd076cfdfe3b 2500w" data-optimize="true" data-opv="2" />

You can see more examples in our [AG-UI integration examples](/examples/applications/ag-ui) section.

## Core Components

* `AGUIApp`: Wraps Agno agents/teams for in a FastAPI app.
* `serve`: Serves the FastAPI AG-UI app using Uvicorn.

`AGUIApp` uses helper functions for routing.

## `AGUIApp` Class

Main entry point for Agno AG-UI apps.

### Initialization Parameters

| Parameter     | Type                       | Default | Description                                      |
| ------------- | -------------------------- | ------- | ------------------------------------------------ |
| `agent`       | `Optional[Agent]`          | `None`  | Agno `Agent` instance.                           |
| `team`        | `Optional[Team]`           | `None`  | Agno `Team` instance.                            |
| `settings`    | `Optional[APIAppSettings]` | `None`  | API configuration. Defaults if `None`.           |
| `api_app`     | `Optional[FastAPI]`        | `None`  | Existing FastAPI app. New one created if `None`. |
| `router`      | `Optional[APIRouter]`      | `None`  | Existing APIRouter. New one created if `None`.   |
| `app_id`      | `Optional[str]`            | `None`  | App identifier (autogenerated if not set).       |
| `name`        | `Optional[str]`            | `None`  | Name for the App.                                |
| `description` | `Optional[str]`            | `None`  | Description for the App.                         |

*Provide `agent` or `team`, not both.*

### Key Method

| Method    | Parameters               | Return Type | Description                                                                                 |
| --------- | ------------------------ | ----------- | ------------------------------------------------------------------------------------------- |
| `get_app` | `use_async: bool = True` | `FastAPI`   | Returns configured FastAPI app (async by default). Sets prefix, error handlers, CORS, docs. |

## Endpoints

Endpoints are available at the specified `prefix` (default `/v1`).

### 1. `POST /agui`

This is the main entrypoint to interact with your Agno Agent or Team.

It expects a `RunAgentInput` object (from the `ag-ui-protocol` package) as defined by the protocol. You can read more about it in [their docs](https://docs.ag-ui.com/quickstart/server).

## Serving the Application (`serve`)

Serves the FastAPI app using Uvicorn.

### Parameters

| Parameter | Type                  | Default       | Description                                       |
| --------- | --------------------- | ------------- | ------------------------------------------------- |
| `app`     | `Union[str, FastAPI]` | `N/A`         | FastAPI app instance or import string (Required). |
| `host`    | `str`                 | `"localhost"` | Host to bind.                                     |
| `port`    | `int`                 | `7777`        | Port to bind.                                     |
| `reload`  | `bool`                | `False`       | Enable auto-reload for development.               |

You can check some usage examples in our [AG-UI integration examples](/examples/applications/ag-ui) section.

You can also check the [CopilotKit docs](https://docs.copilotkit.ai/agno) on working with Agno, to learn more on how to build the UI side.


# Discord Bot
Source: https://docs.agno.com/applications/discord/introduction

Host agents as Discord Bots.

The Discord Bot integration allows you to serve Agents or Teams via Discord, using the discord.py library to handle Discord events and send messages.

## Setup Steps

<Snippet file="setup-discord-app.mdx" />

### Example Usage

Create an agent, wrap it with `DiscordClient`, and run it:

```python
from agno.agent import Agent
from agno.app.discord import DiscordClient
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"), 
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
)

discord_agent = DiscordClient(basic_agent)
if __name__ == "__main__":
    discord_agent.serve()
```

## Core Components

* `DiscordClient`: Wraps Agno agents/teams for Discord integration using discord.py.
* `DiscordClient.serve`: Starts the Discord bot client with the provided token.

## `DiscordClient` Class

Main entry point for Agno Discord bot applications.

### Initialization Parameters

| Parameter | Type              | Default | Description            |
| --------- | ----------------- | ------- | ---------------------- |
| `agent`   | `Optional[Agent]` | `None`  | Agno `Agent` instance. |
| `team`    | `Optional[Team]`  | `None`  | Agno `Team` instance.  |

*Provide `agent` or `team`, not both.*

## Event Handling

The Discord bot automatically handles various Discord events:

### Message Events

* **Description**: Processes all incoming messages from users
* **Media Support**: Handles images, videos, audio files, and documents
* **Threading**: Automatically creates threads for conversations
* **Features**:
  * Automatic thread creation for each conversation
  * Media processing and forwarding to agents
  * Message splitting for responses longer than 1500 characters
  * Support for reasoning content display
  * Context enrichment with username and message URL

### Supported Media Types

* **Images**: Direct URL processing for image analysis
* **Videos**: Downloads and processes video content
* **Audio**: URL-based audio processing
* **Files**: Downloads and processes document attachments

## Environment Variables

Ensure the following environment variable is set:

```bash
export DISCORD_BOT_TOKEN="your-discord-bot-token"
```

## Message Processing

The bot processes messages with the following workflow:

1. **Message Reception**: Receives messages from Discord channels
2. **Media Processing**: Downloads and processes any attached media
3. **Thread Management**: Creates or uses existing threads for conversations
4. **Agent/Team Execution**: Forwards the message and media to the configured agent or team
5. **Response Handling**: Sends the response back to Discord, splitting long messages if necessary
6. **Reasoning Display**: Shows reasoning content in italics if available

## Features

### Automatic Thread Creation

* Creates a new thread for each user's first message
* Maintains conversation context within threads
* Uses the format: `{username}'s thread`

### Media Support

* **Images**: Passed as `Image` objects with URLs
* **Videos**: Downloaded and passed as `Video` objects with content
* **Audio**: Passed as `Audio` objects with URLs
* **Files**: Downloaded and passed as `File` objects with content

### Message Formatting

* Long messages (>1500 characters) are automatically split
* Reasoning content is displayed in italics
* Batch numbering for split messages: `[1/3] message content`

## Testing the Integration

1. Set up your Discord bot token: `export DISCORD_BOT_TOKEN="your-token"`
2. Run your application: `python your_discord_bot.py`
3. Invite the bot to your Discord server
4. Send a message in any channel where the bot has access
5. The bot will automatically create a thread and respond


# FastAPI App
Source: https://docs.agno.com/applications/fastapi/introduction

Host agents as FastAPI Applications.

The FastAPI App is used to serve Agents or Teams using a FastAPI server with a rest api interface.

### Example Usage

Create an agent, wrap it with `FastAPIApp`, and serve it:

```python
from agno.agent import Agent
from agno.app.fastapi.app import FastAPIApp
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    agent_id="basic_agent",
    model=OpenAIChat(id="gpt-4o"), # Ensure OPENAI_API_KEY is set
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

# Async router by default (use_async=True)
fastapi_app = FastAPIApp(
    agents=[basic_agent],
    name="Basic Agent",
    app_id="basic_agent",
    description="A basic agent that can answer questions and help with tasks.",
)

app = fastapi_app.get_app()

# For synchronous router:
# app = fastapi_app.get_app(use_async=False)

if __name__ == "__main__":
    fastapi_app.serve(app="basic:app", port=8001, reload=True)

```

**To run:**

1. Set `OPENAI_API_KEY` environment variable.
2. API at `http://localhost:8001`, docs at `http://localhost:8001/docs`.

Send `POST` requests to `http://localhost:8001/runs?agent_id=basic_agent`:

```curl
curl -s -X POST "http://localhost:8001/runs?agent_id=basic_agent" \
   --header 'Content-Type: application/x-www-form-urlencoded' \
   --data-urlencode  'message=Hello!' | jq -r .content
```

## Core Components

* `FastAPIApp`: Wraps Agno agents/teams for FastAPI.
* `FastAPIApp.serve`: Serves the FastAPI app using Uvicorn.

`FastAPIApp` uses helper functions for routing.

## `FastAPIApp` Class

Main entry point for Agno FastAPI apps.

### Initialization Parameters

| Parameter     | Type                       | Default | Description                                      |
| ------------- | -------------------------- | ------- | ------------------------------------------------ |
| `agents`      | `Optional[List[Agent]]`    | `None`  | List of Agno `Agent` instances.                  |
| `teams`       | `Optional[List[Team]]`     | `None`  | List of Agno `Team` instances.                   |
| `workflows`   | `Optional[List[Team]]`     | `None`  | List of Agno `Workflow` instances.               |
| `settings`    | `Optional[APIAppSettings]` | `None`  | API configuration. Defaults if `None`.           |
| `api_app`     | `Optional[FastAPI]`        | `None`  | Existing FastAPI app. New one created if `None`. |
| `router`      | `Optional[APIRouter]`      | `None`  | Existing APIRouter. New one created if `None`.   |
| `app_id`      | `Optional[str]`            | `None`  | App identifier (autogenerated if not set).       |
| `name`        | `Optional[str]`            | `None`  | Name for the App.                                |
| `description` | `Optional[str]`            | `None`  | Description for the App.                         |

*Provide `agent` or `team`, not both.*

### Key Method

| Method    | Parameters                                          | Return Type | Description                                                                                 |
| --------- | --------------------------------------------------- | ----------- | ------------------------------------------------------------------------------------------- |
| `get_app` | `use_async: bool = True`<br />`prefix: str = "/v1"` | `FastAPI`   | Returns configured FastAPI app (async by default). Sets prefix, error handlers, CORS, docs. |

## Endpoints

Endpoints are available at the specified `prefix` (default `/v1`).

### 1. `POST /run`

* **Description**: Interacts with the agent/team (uses `agent.run()`/`arun()` or `team.run()`/`arun()`).
* **Request Form Parameters**:
  | Parameter    | Type                         | Default                                | Description                             |
  | ------------ | ---------------------------- | -------------------------------------- | --------------------------------------- |
  | `message`    | `str`                        | `...`                                  | Input message (Required).               |
  | `stream`     | `bool`                       | `True` (sync), `False` (async default) | Stream response.                        |
  | `monitor`    | `bool`                       | `False`                                | Enable monitoring.                      |
  | `session_id` | `Optional[str]`              | `None`                                 | Session ID for conversation continuity. |
  | `user_id`    | `Optional[str]`              | `None`                                 | User ID.                                |
  | `files`      | `Optional[List[UploadFile]]` | `None`                                 | Files to upload.                        |
* **Responses**:
  * `stream=True`: `StreamingResponse` (`text/event-stream`) with JSON `RunResponse`/`TeamRunResponse` events.
  * `stream=False`: JSON `RunResponse`/`TeamRunResponse` dictionary.

### Parameters

| Parameter | Type                  | Default       | Description                                       |
| --------- | --------------------- | ------------- | ------------------------------------------------- |
| `app`     | `Union[str, FastAPI]` | `N/A`         | FastAPI app instance or import string (Required). |
| `host`    | `str`                 | `"localhost"` | Host to bind.                                     |
| `port`    | `int`                 | `7777`        | Port to bind.                                     |
| `reload`  | `bool`                | `False`       | Enable auto-reload for development.               |


# Playground App
Source: https://docs.agno.com/applications/playground/introduction

Host agents as Playground Applications.

The Playground App is used to serve Agents, Teams and Workflows using a FastAPI server with several endpoints to manage and interact with `Agents`, `Workflows`, and `Teams` on the [Agno Playground](/introduction/playground).

### Example Usage

Create an agent, and serve it with `Playground`:

```python
from agno.agent import Agent
from agno.memory.agent import AgentMemory
from agno.memory.db.postgres import PgMemoryDb
from agno.models.openai import OpenAIChat
from agno.playground import Playground
from agno.storage.postgres import PostgresStorage

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"), # Ensure OPENAI_API_KEY is set
    memory=AgentMemory(
        db=PgMemoryDb(
            table_name="agent_memory",
            db_url=db_url,
        ),
        create_user_memories=True,
        update_user_memories_after_run=True,
        create_session_summary=True,
        update_session_summary_after_run=True,
    ),
    storage=PostgresStorage(
        table_name="agent_sessions", db_url=db_url, auto_upgrade_schema=True
    ),
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
    markdown=True,
)

playground = Playground(
    agents=[
        basic_agent,
    ],
    name="Basic Agent",
    description="A playground for basic agent",
    app_id="basic-agent",
)
app = playground.get_app()

if __name__ == "__main__":
    playground.serve(app="basic:app", reload=True)
```

**To run:**

1. Ensure your PostgreSQL server is running and accessible via the `db_url`.
2. Set the `OPENAI_API_KEY` environment variable.
3. The Playground UI will be available at `http://localhost:7777`. API docs (if enabled in settings) are typically at `http://localhost:7777/docs`.
4. Use playground with [Agent Playground](/introduction/playground) .

## Core Components

* `Playground`: Wraps Agno agents, teams, or workflows in an API.
* `Playground.serve`: Serves the Playground FastAPI app using Uvicorn.

The `Playground` class is the main entry point for creating Agno Playground applications. It allows you to easily expose your agents, teams, and workflows through a web interface with [Agent Playground](/introduction/playground) or  [Agent UI](/agent-ui/introduction).

## `Playground` Class

### Initialization Parameters

| Parameter     | Type                           | Default | Description                                           |
| ------------- | ------------------------------ | ------- | ----------------------------------------------------- |
| `agents`      | `Optional[List[Agent]]`        | `None`  | List of Agno `Agent` instances.                       |
| `teams`       | `Optional[List[Team]]`         | `None`  | List of Agno `Team` instances.                        |
| `workflows`   | `Optional[List[Workflow]]`     | `None`  | List of Agno `Workflow` instances.                    |
| `settings`    | `Optional[PlaygroundSettings]` | `None`  | Playground configuration. Defaults if `None`.         |
| `api_app`     | `Optional[FastAPI]`            | `None`  | Existing FastAPI app. A new one is created if `None`. |
| `router`      | `Optional[APIRouter]`          | `None`  | Existing APIRouter. A new one is created if `None`.   |
| `app_id`      | `Optional[str]`                | `None`  | App identifier (autogenerated if not set).            |
| `name`        | `Optional[str]`                | `None`  | Name for the App.                                     |
| `description` | `Optional[str]`                | `None`  | Description for the App.                              |

*Provide at least one of `agents`, `teams`, or `workflows`.*

### Key Methods

| Method             | Parameters                                          | Return Type | Description                                                                                 |
| ------------------ | --------------------------------------------------- | ----------- | ------------------------------------------------------------------------------------------- |
| `get_app`          | `use_async: bool = True`<br />`prefix: str = "/v1"` | `FastAPI`   | Returns configured FastAPI app (async by default). Sets prefix, error handlers, CORS, docs. |
| `get_router`       |                                                     | `APIRouter` | Returns the synchronous APIRouter for playground endpoints.                                 |
| `get_async_router` |                                                     | `APIRouter` | Returns the asynchronous APIRouter for playground endpoints.                                |

### Endpoints

Endpoints are available at the specified `prefix` (default `/v1`) combined with the playground router's prefix (`/playground`). For example, the status endpoint is typically `/v1/playground/status`.

### Parameters

| Parameter | Type                  | Default       | Description                                       |
| --------- | --------------------- | ------------- | ------------------------------------------------- |
| `app`     | `Union[str, FastAPI]` | `N/A`         | FastAPI app instance or import string (Required). |
| `host`    | `str`                 | `"localhost"` | Host to bind.                                     |
| `port`    | `int`                 | `7777`        | Port to bind.                                     |
| `reload`  | `bool`                | `False`       | Enable auto-reload for development.               |


# Slack App
Source: https://docs.agno.com/applications/slack/introduction

Host agents as Slack Applications.

The Slack App is used to serve Agents or Teams via Slack, using a FastAPI server to handle Slack events and send messages.

## Setup Steps

<Snippet file="setup-slack-app.mdx" />

### Example Usage

Create an agent, wrap it with `SlackAPI`, and serve it:

```python
from agno.agent import Agent
from agno.app.slack.app import SlackAPI
from agno.models.openai import OpenAIChat

basic_agent = Agent(
    name="Basic Agent",
    model=OpenAIChat(id="gpt-4o"), # Ensure OPENAI_API_KEY is set
    add_history_to_messages=True,
    num_history_responses=3,
    add_datetime_to_instructions=True,
)

slack_api_app = SlackAPI(
    agent=basic_agent,
)
app = slack_api_app.get_app()

if __name__ == "__main__":
    slack_api_app.serve("basic:app", port=8000, reload=True)
```

## Core Components

* `SlackAPI`: Wraps Agno agents/teams for Slack integration via FastAPI.
* `SlackAPI.serve`: Serves the FastAPI app using Uvicorn, configured for Slack.

## `SlackAPI` Class

Main entry point for Agno Slack applications.

### Initialization Parameters

| Parameter     | Type                       | Default | Description                                      |
| ------------- | -------------------------- | ------- | ------------------------------------------------ |
| `agent`       | `Optional[Agent]`          | `None`  | Agno `Agent` instance.                           |
| `team`        | `Optional[Team]`           | `None`  | Agno `Team` instance.                            |
| `settings`    | `Optional[APIAppSettings]` | `None`  | API configuration. Defaults if `None`.           |
| `api_app`     | `Optional[FastAPI]`        | `None`  | Existing FastAPI app. New one created if `None`. |
| `router`      | `Optional[APIRouter]`      | `None`  | Existing APIRouter. New one created if `None`.   |
| `app_id`      | `Optional[str]`            | `None`  | App identifier (autogenerated if not set).       |
| `name`        | `Optional[str]`            | `None`  | Name for the App.                                |
| `description` | `Optional[str]`            | `None`  | Description for the App.                         |

*Provide `agent` or `team`, not both.*

## Endpoints

The main endpoint for Slack integration:

### `POST /slack/events`

* **Description**: Handles all Slack events including messages and app mentions
* **Security**: Verifies Slack signature for each request
* **Event Types**:
  * URL verification challenges
  * Message events
  * App mention events
* **Features**:
  * Threaded conversations
  * Background task processing
  * Message splitting for long responses
  * Support for both direct messages and channel interactions

## Testing the Integration

1. Start your application locally with `python <my-app>.py` (ensure ngrok is running)
2. Invite the bot to a channel using `/invite @YourAppName`
3. Try mentioning the bot in the channel: `@YourAppName hello`
4. Test direct messages by opening a DM with the bot

## Troubleshooting

* Verify all environment variables are set correctly
* Ensure the bot has proper permissions and is invited to channels
* Check ngrok connection and URL configuration
* Verify event subscriptions are properly configured
* Monitor application logs for detailed error messages

## Support

For additional help or to report issues, please refer to the documentation or open an issue in the repository.


# Whatsapp App
Source: https://docs.agno.com/applications/whatsapp/introduction

Host agents as Whatsapp Applications.

The Whatsapp App is used to serve Agents or Teams interacting via WhatsApp, using a FastAPI server to handle webhook events and to send messages.

<Snippet file="setup-whatsapp-app.mdx" />

### Example Usage

Create an agent, wrap it with `WhatsappAPI`, and serve it:

```python
from agno.agent import Agent
from agno.app.whatsapp.app import WhatsappAPI
from agno.models.openai import OpenAIChat
from agno.tools.openai import OpenAITools

image_agent = Agent(
    model=OpenAIChat(id="gpt-4o"), # Ensure OPENAI_API_KEY is set
    tools=[OpenAITools(image_model="gpt-image-1")],
    markdown=True,
    show_tool_calls=True,
    debug_mode=True,
    add_history_to_messages=True,
)

# Async router by default (use_async=True)
whatsapp_app = WhatsappAPI(
    agent=image_agent,
    name="Image Generation Tools",
    app_id="image_generation_tools",
    description="A tool that generates images using the OpenAI API.",
)

app = whatsapp_app.get_app()

if __name__ == "__main__":
    whatsapp_app.serve(app="image_generation_tools:app", port=8000, reload=True)
```

**To run:**

1. Ensure `OPENAI_API_KEY` environment variable is set if using OpenAI models.
2. The API will be running (e.g., `http://localhost:8000`), but interaction is primarily via WhatsApp through the configured webhook.
3. API docs (if enabled in settings) might be at `http://localhost:8000/docs`.

## Core Components

* `WhatsappAPI`: Wraps Agno agents/teams for WhatsApp integration via FastAPI.
* `WhatsappAPI.serve`: Serves the FastAPI app using Uvicorn, configured for WhatsApp.

## `WhatsappAPI` Class

Main entry point for Agno WhatsApp applications.

### Initialization Parameters

| Parameter     | Type                       | Default | Description                                      |
| ------------- | -------------------------- | ------- | ------------------------------------------------ |
| `agent`       | `Optional[Agent]`          | `None`  | Agno `Agent` instance.                           |
| `team`        | `Optional[Team]`           | `None`  | Agno `Team` instance.                            |
| `settings`    | `Optional[APIAppSettings]` | `None`  | API configuration. Defaults if `None`.           |
| `api_app`     | `Optional[FastAPI]`        | `None`  | Existing FastAPI app. New one created if `None`. |
| `router`      | `Optional[APIRouter]`      | `None`  | Existing APIRouter. New one created if `None`.   |
| `app_id`      | `Optional[str]`            | `None`  | App identifier (autogenerated if not set).       |
| `name`        | `Optional[str]`            | `None`  | Name for the App.                                |
| `description` | `Optional[str]`            | `None`  | Description for the App.                         |

*Provide `agent` or `team`, not both.*

### Key Method

| Method    | Parameters                                       | Return Type | Description                                                                                                                  |
| --------- | ------------------------------------------------ | ----------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `get_app` | `use_async: bool = True`<br />`prefix: str = ""` | `FastAPI`   | Returns configured FastAPI app. Sets prefix, error handlers, and includes WhatsApp routers. Async router is used by default. |

## Endpoints

Endpoints are accessible at the `prefix` (default is root level: `""`).

### 1. `GET /webhook`

* **Description**: Verifies WhatsApp webhook (challenge).
* **Responses**:
  * `200 OK`: Returns `hub.challenge` if tokens match.
  * `403 Forbidden`: Token mismatch or invalid mode.
  * `500 Internal Server Error`: `WHATSAPP_VERIFY_TOKEN` not set.

### 2. `POST /webhook`

* **Description**: Receives incoming WhatsApp messages and events.
* **Processing**:
  * Validates signature (if `APP_ENV="production"` and `WHATSAPP_APP_SECRET` is set).
  * Processes messages (text, image, video, audio, document) via `agent.arun()` or `team.arun()`.
  * Sends replies via WhatsApp.
* **Responses**:
  * `200 OK`: `{"status": "processing"}` or `{"status": "ignored"}`.
  * `403 Forbidden`: Invalid signature.
  * `500 Internal Server Error`: Other processing errors.

### Parameters

| Parameter | Type                  | Default       | Description                                       |
| --------- | --------------------- | ------------- | ------------------------------------------------- |
| `app`     | `Union[str, FastAPI]` | `N/A`         | FastAPI app instance or import string (Required). |
| `host`    | `str`                 | `"localhost"` | Host to bind.                                     |
| `port`    | `int`                 | `7777`        | Port to bind.                                     |
| `reload`  | `bool`                | `False`       | Enable auto-reload for development.               |


# Product updates
Source: https://docs.agno.com/changelog/overview



<Update label="2025-08-26" description="v1.8.0">
  ## New Features:

  * **Memori ToolKit:** Added **`MemoriTools`**, a ToolKit for Agents and Teams to use GibsonAI’s Memori.

  ## Improvements:

  * **Workflows File Input**: Now `files=[]` can be provided to `Workflow.run` or `Workflow.arun`.
  * **Scrapegraph Tools Agentic Crawling**: Added `agentic_crawler` to `ScrapeGraphTools`.
  * **VertexAI Search:** Added Vertex AI Search to the Gemini model.

  ## Updates:

  * **DuckDuckGoTools**: Updated `DuckDuckGoTools` to work with the `ddgs` package. This requires users to install `ddgs` instead of `duckduckgo-search`.
</Update>

<Update label="2025-08-21" description="v1.7.12">
  ## Improvements:

  * **Teams Collaborate Streaming:** Added support for streaming or teams on `collaborate` mode.
  * **OpenAI Verbosity**: Added `verbosity` parameter to `OpenAIChat` and `OpenAIResponses`.
  * **Gemini URL Context Tool:** Adds support fior Gemini URL context tool.

  ## Bug Fixes:

  * **GCSKnowledgeBase Load:** Fixed cases where `.load()` functioned incorrectly.
  * **Youtube Transcripts**: Removed deprecated implementation in `YoutubeTools`.
</Update>

<Update label="2025-08-17" description="v1.7.11">
  ## New Features:

  * **In Memory Storage**: Added `InMemoryStorage` for a simpler storage option useful in rapid prototyping.
  * **Trafilatura Tools**: Added `TrafilaturaTools` SDK for web scraping and text extraction using the Trafilatura library, with 15 comprehensive cookbook examples.
  * **Qwen and Dashscope Native Models**: Use Qwen models via `DashScope` integration.
  * **Brandfetch Tools**: Added tools allowing agents to fetch brand information and assets via Brandfetch.

  ## Improvements:

  * **Async Support for Milvus Hybrid Search**
  * **Bedrock Files Support**: Now supporting `File` for compatible AWS Bedrock models.

  ## Bug Fixes:

  * **MarkdownKnowledgeBase Custom Chunking Strategy**: Fixed issues with setting a custom chunking strategy.
  * **Teams Routing/Coordination System Message**: Added criteria for when the team must respond directly or forward requests to its members.
</Update>

<Update label="2025-08-12" description="v1.7.10">
  ## New Features:

  * **GPT-5 Compatibility**: Support for GPT-5 with our `OpenAIResponses` class

  ## Improvements:

  * **Knowledge Base PDF with Password Support**: Added support for password-protected PDFs on `PDFKnowledgeBase`.
  * **Pagination for GitHub Tools:**  Added pagination with metadata for GitHub Tools.
  * **Team** **Role Parameter:** Added a `role` parameter to the `Team` class for defining the team's purpose and specialization.

  ## Bug fixes:

  * Support for all GPT reasoning models with our `OpenAIResponses` class
</Update>

<Update label="2025-08-08" description="v1.7.9">
  ## **Improvements:**

  * **Reranker in Hybrid Search**: Added support for Reranker in Pgvector hybrid search
  * **Output Model**: Added support for `stream_intermediate_steps` with output model
  * **PDF page split**: Refactor of PDF Readers and addition of Page Number Handling

  ## **Bug Fixes:**

  * **LanceDb Remote Connection**: Fixed a bug preventing connection to a remote LanceDb connection.
  * **Messages**: Fixed the order when using both `message` and `messages`.
</Update>

<Update label="2025-08-07" description="v1.7.8">
  ## Improvements:

  * **Support for OpenAI Flex Processing**: Added `service_tier` to `OpenAIChat` and `OpenAIResponses`.

  ## Bug Fixes:

  * **Print Response:**
    * Fixed `show_member_responses` not working correctly on `Team`
    * Fixed printing of MCP responses on streamable HTTP
  * **Session State on Team**: Fixed precedence for session state from sessions DB.
  * **`YouTubeTranscriptApi` has no attribute `get_transcript` :**
    * The `YoutubeTranscriptApi` is updated and now uses `.fetch(video_id)` for getting transcripts.
  * **HITL streaming:**
    * Added the required attributes- `tools_requiring_confirmation`, `tools_requiring_user_input`, `tools_awaiting_external_execution` on the class `BaseAgentRunResponseEvent`
    * If you are using streaming, it is recommended to pass the `run_id` and a list of `updated_tools` to the `continue_run` or `acontinue_run` method.
</Update>

<Update label="2025-08-01" description="v1.7.7">
  ## New Features:

  * **Updated MCP Tools:** Our \*\*\*\*`MCPTools` and `MultiMCPTools` classes can now be initialized and used without an async context manager, providing a much easier experience. See the updated [docs](https://docs.agno.com/tools/mcp/mcp).
  * **Morph Tools:** Morph’s Fast Apply model as a Tool, it intelligently merges your original code with update snippets at 98% accuracy and 4500+ tokens/second.

  ## Improvements:

  * **LiteLLM File & Image Understanding**: Added support for file and image inputs for `LiteLLM`.
  * **Upgrade ZepTools to v3 Zep**: `ZepTools` are now compatible with Zep v3.

  ## Bug Fixes:

  * **OpenAIEmbedder() wrong dimensions for text-embedding-3-large:** Automatically handles default `dimensions` length for `text-embedding-3-small` as 1536 and `text-embedding-3-large` as 3072.
</Update>

<Update label="2025-07-24" description="v1.7.6">
  ## New Features:

  * **Portkey Model Support**: Added support for Portkey hosted models.
  * **Bitbucket Tool**: Added `BitbucketTools` with a variety of Bitbucket repository actions.
  * **Jina Embedder:** Added `JinaEmbedder` for using embedding models via Jina.
  * **Row Chunking**: Added `RowChunking` as a CSV chunking strategy.
  * **EVM Toolkit**: Added `EvmTools` to do transactions on EVM compatible blockchains using `web3`.
  * **LinkUp Toolkit**: Added `LinkupTools` for powerful search.
  * **Background Execution Support for Workflows 2.0:** Introduced background execution capabilities for Workflows 2.0, enabling non-blocking workflow execution with polling support. See docs [here](https://docs.agno.com/workflows_2/background-execution).

  ## Improvements:

  * **Async Bedrock Support**: Added async execution support for the AWS bedrock implementation.
  * **PostgreSQL Tools Updates:** Various security and stability overhauls made to the `PostgresTools` toolkit.
  * **Daytona Toolkit Updates:** Added new tools for `Daytona`  agent Toolkit

  ## Bug Fixes:

  * **LiteLLM Metrics**: Fixed issue with metrics on streaming responses from LiteLLM.
  * **Team Expected Output**: Fixed issue where expected\_output of members were overwritten by the team leader agent.
  * **Workflows Async Generators**: Fixed how async generator `arun` functions are treated. It now correctly keeps async generators as async generators and doesn't convert it to a coroutine.
    * **Before:** Workflows with Async generator `arun` functions were incorrectly awaited as coroutines, which could cause runtime errors or prevent proper iteration through the yielded asynchronous values.
    * **After:** Async generator workflows are now properly recognised and handled as async generators, allowing for correct iteration over their yielded values using `async for`. This ensures all yielded results are processed as intended within asynchronous workflows.
  * **LiteLLM Multiple Streaming Tool Calls:** When Agno is run through LiteLLM against OpenAI chat models (eg. GPT4.1), multiple streamed tool\_calls lost their individual argument streams. This has been resolved.
</Update>

<Update label="2025-07-17" description="v1.7.5">
  ## New Features:

  * **SurrealDB:** Added SurrealDB support as a vector DB for knowledge bases.

  ## Improvements:

  * **Session Caching:** Added `cache_session` attribute to allow users to switch off session caching, which improves on memory management.
  * **Workflows 2.0 FastAPI Support**: Added support for running the new workflows in `FastAPIApp` .

  ## Bug Fixes:

  * **Nested Tool Hooks**: Fixed bug with nested tool hooks.
</Update>

<Update label="2025-07-16" description="v1.7.4">
  ## Workflows 2.0

  <img height="200" src="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=69c14e275faab1eec2997ff2533b1c8c" width="868" height="237" data-path="images/changelogs/release-1-7-4.png" srcset="https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?w=280&maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=51d86571ea10f7815ed30f7b2cb3a14f 280w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?w=560&maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=ec46508abf80458a4d3aa3cb06b3b0fd 560w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?w=840&maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=e4203405bff18d25f6286657a8574d0e 840w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?w=1100&maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=42336cd9c0c1e453c49b2b3385b8128a 1100w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?w=1650&maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=1f062f7afe7fd136c26962d8da2a39bd 1650w, https://mintcdn.com/agno/0omqNXZ9CSqcNU7_/images/changelogs/release-1-7-4.png?w=2500&maxW=868&auto=format&n=0omqNXZ9CSqcNU7_&q=85&s=3140bb95d04850253b484c0ec3af143d 2500w" data-optimize="true" data-opv="2" />

  ## New Features:

  * **Workflows Revamped (beta):** Our new Workflows implementation (internally referred to as Workflows 2.0) is a complete redesign of the workflow system in Agno, introducing a step-based architecture that provides better structure, flexibility, and control over multi-stage AI agent processes
    * Core Concepts:
      * **Flexible Execution**: Sequential, parallel, conditional, and loop-based execution
      * **Smart Routing**: Dynamic step selection based on content analysis or user intent
      * **Mixed Components**: Combine agents, teams, and functions seamlessly
      * **State Management:** Share data across steps with session state
    * For a comprehensive guide to the new Workflows system, check out the [docs](https://docs.agno.com/workflows_2/overview).
  * **Pydantic Input for Agents/Teams:** Both Agent and Team now accepts structured input (i.e. a pydantic model) on run() and print\_response().
</Update>

<Update label="2025-07-15" description="v1.7.3">
  ## New Features:

  * **Session State on Run**: You can now pass `session_state` when running an agent or team. See [docs](https://docs.agno.com/agents/state) for more information.
  * **GCS Support for PDF Knowledge Base:** Added `GCSPDFKnowledgeBase` to support PDFs on Google Cloud Storage.

  ## Bug Fixes:

  * **Workflows Async + Storage**: Fixed issues where sessions was not correctly stored with async workflow executions.
  * **Session State Management**: Fixed issues around session state management in teams and agents. Session state and session name will now correctly reset and load from storage if sessions are switched.
  * **Metadata Support for Document Knowledge Base:** Adds metadata support for `DocumentKnowledgeBase`
  * **Session Metrics with History**: Fixed bug with session metrics on `Agent` where history is enabled.
</Update>

<Update label="2025-07-10" description="v1.7.2">
  ## New Features:

  * **MySQL Storage**: Added support for `MySQLStorage` as an agent/team/workflow session storage backend.
  * **XAi Live Search**: Added support for live search on the XAi model provider.
  * **OpenAI Deep Research**: Support added for `o4-mini-deep-research` and `o3-deep-research` models.
  * **Stagehand MCP example**: Added an example of using Stagehand MCP with Agno.
  * **Atla observability example**: Added an example of using Atla observability with Agno.
  * **Scrapegraph example**: Added an example of using Scrapegraph with Agno.

  ## Improvements:

  * **Memory Growth on Performance Evals**: Added `memory_growth_tracking` as an attribute on `PerformanceEval` to enable additional debug logs for memory growth.
  * **Agent/Team in Tool Hook**: Added `agent` and `team` as optional parameters in tool hooks, for more flexibility.

  ## Bug Fixes:

  * **Gemini 2.5 Metrics:** Fixed Gemini metrics to correctly include “thinking” tokens.
  * **Claude tool calling:** Fixed a bug related to parsing function call responses when using Claude models.
  * **Team Metrics**: Fixed a bug with team metrics when teams have history enabled.
</Update>

<Update label="2025-06-30" description="v1.7.1">
  ## New Features:

  * **Debug Level:** Added `debug_level` to both `Agent` and `Team`. This is an `int` value of either `1` (default) or `2` (more verbose logging). Currently this only enables more verbose model logs, but will be used more widely in future.

  ## Improvements:

  * **Parser Model on Teams**: Added `parser_model` support for `Team`. See docs [here](https://docs.agno.com/teams/structured-output#using-a-parser-model).
  * **Support for Gemini Thinking**: Added `thinking_budget` and `include_thoughts` parameters for `Gemini` model class.
  * **Serper Tools**: Made updates to the toolkit to include new tools `search_news` , `search_scholar` and `scrape_webpage`.
  * **Valyu Tools:** New Valyu toolkit for Deep Search capabilities of academic sources.
  * **Oxylabs:** Added `OxylabsTools` for adding more web-scraping capabilities to agents.

  ## Bug Fixes:

  * **DuckDB CSV parsing error:** For CSV files use the custom `read_csv` method for improved CSV parsing
  * **Full Team Metrics**: Fixed an issue with calculation of the `full_team_session_metrics` on Teams.
</Update>

<Update label="2025-06-26" description="v1.7.0">
  ## New Features:

  * **Agent/Team Add Tool**: Added convenience function to `Agent` and `Team` → `add_tool(tool)` to append new tools after inititialisation.
  * **Streaming Structured Output**: Implemented structured output during streaming. This means streaming won’t be turned off when `response_model` is passed. The structured output itself is not streamed, but it is part of the iterator response when running an agent/team with streaming. The response model is set on a single `RunResponseContentEvent` and on the final `RunResponseCompletedEvent`.

  ## Improvements:

  * **Linear Teams Tool**: Added tool to get the list of teams from Linear.

  ## Bug Fixes:

  * **Uppercase Structured Output**: Resolved cases where pydantic model fields contain field names with upper case characters.

  ## Breaking Change:

  * If you use `run(..., stream=True)` or `arun(..., stream=True)` on `Agent` or `Team` with a `response_model` set, the current behaviour would switch off streaming and respond with a single `RunResponse` object. After the changes mentioned above, you will get the `Iterator[RunResponseEvent]` / `AsyncIterator[RunResponseEvent]` response instead.
</Update>

<Update label="2025-06-23" description="v1.6.4">
  ## New Features:

  * **Brightdata Toolkit**: Added multiple web-based tools via Brightdata.
  * **OpenCV Video/Image Toolkit**: Added tools for capturing image/video via your webcam.
  * **DiscordClient App**: Added a DiscordClient app for connecting your agent or team with Discord in the form of a discord bot.

  ## Improvements:

  * **FileTools File Search**: Added `search` to `FileTools`.

  ## Bug Fixes:

  * **Fix User Control Flow with History**: Fixed issues where user control flow (HITL) flows failed with message history.
  * **Fix lance db upsert method for supporting knowledge filters:** Fixed function `upsert` not using the parameter `filters`
  * **Update mongo db hybrid search for filters:** Mongodb now correctly uses filters for hybrid search
  * **Fixed `team.rename_session(...)` that raises an `AttributeError`: Fixed** function `rename_session` not using the parameter `session_id`
</Update>

<Update label="2025-06-18" description="v1.6.3">
  ## New Features:

  * **User Control Flows on Playground**: The Agno Platform now support user control flows on the playground.
  * **Team & Agent Events on RunResponse:** Added `store_events` parameter to optionally add all events that happened during the course of an agent/team run on the `RunResponse`/`TeamRunResponse`.
  * **Team Member Responses on Playground**: The Agno Platform now shows member responses during team runs.
  * **Behind-the-scenes on Playground**: The Agno Platform now shows what is happening during runs, for both agents and teams.
  * **Metadata filtering support for `csv` and `csv_url` knowledge bases:** Add knowledge filters support for these knowledge base types.

  ## Updates

  * **Removed `a` prefix from async function names:** `asearch_knowledge_base`, etc will now be the same as their `sync` counterparts when sent to the model. The names of functions are important for accurate function calling.

  ## Bug Fixes:

  * **AG-UI Fix**: Fixed issue related to missing messages when using the Agno AG-UI app.
  * **Chat History Fix**: Fixed issue related to history not available when `agent_id` not set.
  * **MongoDB ObjectId serialization issue when using with agent:** Fixed issue while \*\*\*\*using mongodb vectordb with ObjectId present in the metadata it throws Object of type `ObjectId` is not JSON serializable
</Update>

<Update label="2025-06-13" description="v1.6.1">
  ## New Features:

  * **Nebius Embeddings**: Added support for embedding models on Nebius.
  * **Firestore Memory and Storage:** Added support for Firestore both as memory and storage provider for your agents.

  ## Improvements:

  * **Improved Event Payloads**: Added `agent_name` to agent events, and `team_name` to team events. Also added `team_session_id` to team-member events to indicate that it belongs to the top-level team session.
  * **Team Run Events**: Added `stream_member_events` to teams to optionally disable streaming of member events.
  * **DocumentKnowledgeBase Async**: Added `async` support on `DocumentKnowledgeBase`
  * **Enums on Custom Tools**: Added support for `enum` parameters in custom tools.

  ## Bug Fixes:

  * **Team Events:** Fixed issues related to team and member events not being part of the same session. Going forward a team and its members will all have the same session ID.
</Update>

<Update label="2025-06-10" description="v1.6.0">
  ## Improvements:

  * **New Streaming Events**: We have improved our streaming events system. See the details in “breaking changes” section at the bottom.
  * **Member Events in Teams**: The above change includes streaming of events from team members with the top-level team events.

  ## Bug Fixes:

  * **Apify Tools:** Fixed the ApifyTools initialize to correctly register functions.

  ## Breaking Changes:

  * **Updates to Run Without Streaming**:
    * `RunResponse` now does not have an `event` attribute. It still represents the responses of the entire run.
    * An additional attribute `RunResponse.status` now indicates whether the run response is `RUNNING`, `PAUSED`, or `CANCELLED`.
  * **Updates to Run Streaming:**
    * In the case of streaming you now get reformulated run events. These events are streamed if you do `agent.run(..., stream=True)` or `agent.arun(..., stream=True)` .
    * Agents have the following event types:
      * `RunResponseContent`
      * `RunError`
      * `RunCancelled`
      * `ToolCallStarted`
      * `ToolCallCompleted`
      * with `stream_intermediate_steps=True`:
        * `RunStarted`
        * `RunCompleted`
        * `ReasoningStarted`
        * `ReasoningStep`
        * `ReasoningCompleted`
        * `MemoryUpdateStarted`
        * `MemoryUpdateCompleted`
    * See detailed documentation [here](https://docs.agno.com/agents/run).
  * **Updates to Teams:**
    * Teams have the following event types:
      * `TeamRunResponseContent`
      * `TeamRunError`
      * `TeamRunCancelled`
      * `TeamToolCallStarted`
      * `TeamToolCallCompleted`
      * with `stream_intermediate_steps=True`:
        * `TeamRunStarted`
        * `TeamRunCompleted`
        * `TeamReasoningStarted`
        * `TeamReasoningStep`
        * `TeamReasoningCompleted`
        * `TeamMemoryUpdateStarted`
        * `TeamMemoryUpdateCompleted`
    * Teams will also yield events from team members as they are executed.
    * See detailed documentation [here](https://docs.agno.com/teams/run).
  * **Updates to Workflows:**
    * You should now yield `WorkflowRunResponseStartedEvent` and `WorkflowRunResponseCompletedEvent` events.
</Update>

<Update label="2025-06-06" description="v1.5.10">
  ## New Features:

  * **Playground File Upload**: We now support file upload via the Agno Playground. This will send PDF, CSV, Docx, etc files directly to the agents/teams for interpretation by the downstream LLMs.  If you have a knowledge base attached to the agent/team, it will upload the file to the knowledge base instead.
  * **Async Evals**: Support for `async` and evaluations. See examples [here](https://github.com/agno-agi/agno/tree/main/cookbook/evals).

  ## Improvements:

  * **Exa Research**: Added `research` tool on `ExaTools`. See more on their [docs](https://docs.exa.ai/reference/exa-research) about how research works.
  * **Whatsapp Type Indicator**: Add type indicator to Whatsapp responses.

  ## Bug Fixes:

  * **State in Messages Fixes**: Fixed issues around nested json inside messages and adding state into messages.
</Update>

<Update label="2025-06-05" description="v1.5.9">
  ## New Features:

  * **AG-UI App**: Expose your Agno Agents and Teams with an AG-UI compatible FastAPI APP.
  * **vLLM Support**: Added support for running vLLM models via Agno.
  * **Serper Toolkit:** Added `SerperTools` toolkit to search Google
  * **LangDB Support**: Added LangDB AI Gateway support into Agno.
  * **LightRAG server support:** Added LightRAG support which provides a fast, graph-based RAG system that enhances document retrieval and knowledge querying capabilities.
  * **Parser Model:** Added ability to use an external model to apply a structured output to a model response
  * **Pdf Bytes Knowledge: I**ntroduced a new knowledge base class: `PDFBytesKnowledgeBase`, which allows the ingestion of in-memory PDF content via bytes or IO streams instead of file paths.
  * **Qdrant Mcp Server:** Added MCP support for Qdrant
  * **Daytona integration:** Added `DaytonaTools` toolkit to let Agents execute code remotely on Daytona sandboxes
  * **Expand URL addition in Crawl4ai: I**ntroduced a new URL expansion feature directly into the Crawl4ai toolkit. Our agents frequently encounter shortened URLs that crawl4ai cannot scrape effectively, leading to incomplete data retrieval. With this update, shortened URLs are expanded to their final destinations before being processed by the crawler.
  * **AWS SES Tools**: Added `AWSSESTools` to send emails via AWS SES.
  * **Location Aware Agents:** Added `add_location_to_instructions` to automatically detect the current location where the agent is running and add that to the system message.

  ## Improvements:

  * **FastAPIApp Update**: FastAPIApp was updated and `agent` was replaced with `agents`, `team` with `teams` and `workflows` was added. This also now requires you to specify which agent/team/workflow to run.
    * E.g. `http://localhost:8001/runs?agent_id=my-agent`
  * **ZepTools Updates**: Updated `ZepTools` to remove deprecated features.
  * **GmailTools Attachments**: `GmailTools` now support attachments.
  * **Improve code reusability by using fetch with retry and async\_fetch\_with\_retry:** updated `fetch_with_retry` and `async_fetc_with_retry` to be reused in`url_reader.py`
  * **Add name to evaluation examples:** Included name param in evaluation examples
  * **XTools Search**: Added `search_posts` for `XTools`.

  ## Bug Fixes:

  * **Claude Prompt Tokens**: Fixed a bug with prompt tokens not propagating in Claude model class
  * **Add type in base App class for registry:** Can have different types of app like- `slack`, `whatsapp`, etc
  * **Accept empty array of pdf urls:** Fixed an issue where empty PDF URL arrays were not accepted, preventing knowledge base queries without adding new documents
  * **Anthropic cache metrics propagation:** Fixed a bug where Anthropic's prompt caching metrics were not propagating to Agent responses, despite the raw Anthropic API working correctly. This minimal fix ensures cache performance metrics are properly captured and reported.
  * **Handle non serializable objects on RunResponse dict parsing:**
    * Updated `RunResponse.to_dict()` to handle non-serializable fields, as Python enums
</Update>

<Update label="2025-06-03" description="v1.5.8">
  ## New Features:

  * **Slack App**: Introducing the `SlackApp` to allow you to create agents for Slack! The app allows agents to respond to individual messages or on group chats, and it creates threads to respond to messages.
  * **Visualization Tools**: Added `VisualizationTools` that uses `matplotlib` to give agents the ability to make graphs.
  * **Brave Search Tools:** Introduced a new toolkit for integrating `BraveSearch` that allows agent to search the web using brave search api.

  ## Improvements:

  * **Pass filters for traditional RAG:** Properly pass down `knowledge_filters` even if `self.add_references=True` which is a case for traditional RAG (diff from Agentic RAG)
  * **Add infer param to Mem0:** Added infer as a param to `Mem0Tools`

  ## Bug Fixes:

  * **Searxng tool initialization:**  Fixed Searxng tool initialization error

    ```
    AttributeError: 'Searxng' object has no attribute 'include_tools'
    ```

    and added comprehensive unit tests.

  * **Fix for enum as a response model for Gemini:** With 1.5.6, a [bug](https://discord.com/channels/965734768803192842/965734768803192845/1377999191632121926) was introduced now allowing enum as a data type for Gemini response model.

  * **OpenAI parsing structured output:** With the changes in the OpenAI library we don't need to parse the structured output separately.

  * **Fix accuracy evals monitoring:** Added logic to handle monitoring when evaluating Teams in the run function of AccuracyEval

  ## Updates

  * **Updates to Apps:**
    * `FastAPIApp` does not have a default `prefix` anymore and `/run` → `/runs` (i.e. the created run endpoint is now `<your_domain>/runs`)
    * `serve_fastapi_app` is now replaced with `.serve()` on the instance of `FastAPIApp`.
    * `serve_whatsapp_app` is now replaced with `.serve()` on the instance of `WhatsappAPI`.
</Update>

<Update label="2025-05-30" description="v1.5.6">
  ## New Features

  * **Team Evals**: All types of Evaluations now support Teams!

  ## Improvements:

  * **Async Workflows**: Added `arun` support for Workflows, so they can now be used with `async` Python.
  * **Parallel Memory Updates**: Made speed improvements when user memories and session summaries are generated.
  * **Reimplement `tool_call_limit`**: Revamp of `tool_call_limit` to make it work across a whole agent run.
  * **Gemini / OpenAI Structure Response:** Improved Gemini and OpenAI Structured Response support. Dict types can now be used when defining structured responses.

  ## Bug Fixes:

  * **Mistral Structured Outputs with Tools**: Fixed an issue preventing Mistral model use with structured output and tools.
  * **Images In Run Without Prompt**: Fixed issues related to images being ignored if there wasn’t a prompt provided on `run`.
  * **Pgvector Upsert Fix: Fixed** Pgvector upsert not copying metadata properly.
  * **Handle AgnoInstrumentor failing with OpenAIResponses:** PR merged in Arize’s openinference repo: [https://github.com/Arize-ai/openinference/pull/1701](https://github.com/Arize-ai/openinference/pull/1701).
  * **Pinecone Filters:** Enabled filters for pinecone vector db.
  * **Combined KB Async:** Add missing async method to Combined KB.
  * **Team Session State Fix**: **`team_session_state`** is now correctly propagated and shared between all members and sub-teams of a team.
  * **Gemini type fix for integers:**
    * Pydantic models with `Dict[str, int]` fields (and other Dict types) were failing when used as `response_schema` for both OpenAI and Gemini models due to schema format incompatibilities.
  * **Session Name**: `session_name` is now available after a run.
  * **Handle UUIDs while serialization in RedisStorage:** Fixed error object of type UUID is not JSON serializable.

  ## Updates:

  * For managing `team_session_state`, you now have to set `team_session_state` on the `Team` instead of `session_state`.
</Update>

<Update label="2025-05-27" description="v1.5.5">
  ## New Features:

  * **Claude File Upload:** We can now upload a file to Anthropic directly and then use it as an input to an agent.
  * **Claude 4 Code Execution Tool:** Updated Claude to execute Python code in a secure, sandboxed environment.
  * \*\*Prompt caching with Anthropic Models: \*\* Allowed resuming from specific prefixes in your prompts. This approach significantly reduces processing time and costs for repetitive tasks or prompts with consistent elements.
  * **Vercel v0 Model:** Added support for new Vercel v0 models and cookbook examples.
  * **Qdrant Hybrid Search support**
  * **Markdown Knowledge Base**: Added native support for Markdown-based knowledge bases.
  * **AI/ML API platform integration:** Introduced integration with [`AI/ML API`](https://aimlapi.com/models/?utm_source=agno\&utm_medium=github\&utm_campaign=integration), a platform providing AI/ML models. AI/ML API provides 300+ AI models including Deepseek, Gemini, ChatGPT. The models run at enterprise-grade rate limits and uptimes.
  * **Update Pydantic and dataclass in function handling:** Added support for `Pydantic` and `dataclass` objects as input to a function. See [here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/tool_concepts/custom_tools/complex_input_types.py) for an example.

  ## Improvements:

  * **Timeout handling for API calls in ExaTools class:**
    * Timeout functionality to Exa API calls to prevent indefinite hanging of search operations. The implementation uses Python's `concurrent.futures` module to enforce timeouts on all Exa API operations (search, get contents, find similar, and answer generation).
    * This change addresses the issue where Exa search functions would hang indefinitely, causing potential service disruptions and resource leaks.
  * **Fetch messages from last N sessions:**
    * A tool for the agent, something like `get_previous_session_messages(number_of_sessions: int)` that returns a list of messages that the agent can analyse
    * Switch on with `search_previous_sessions_history`
  * **Redis Expiration**: Added `expire` key to set TTL on Redis keys.
  * **Add Anthropic Cache Write to Agent Session Metrics:** Added `cache_creation_input_tokens` to agent session metrics, to allow for tracking Anthropic cache write statistics

  ## Bug Fixes:

  * **Huggingface Embedder Updates:**
    * Huggingface has changed some things on their API and they've deprecated `.post` on their `InferenceClient()`- [https://discuss.huggingface.co/t/getting-error-attributeerror-inferenceclient-object-has-no-attribute-post/156682](https://discuss.huggingface.co/t/getting-error-attributeerror-inferenceclient-object-has-no-attribute-post/156682)
    * We can also no longer use `id: str = "jinaai/jina-embeddings-v2-base-code"` as default, because these models are no longer provided by the `HF Inference API`. Changed the default to `id: str = "intfloat/multilingual-e5-large"`
  * **Add `role_map` for `OpenAIChat`:** This allows certain models that don’t adhere to OpenAI’s role mapping to be used vir `OpenAILike`.
  * **Use Content Hash as ID in Upsert in Pgvector:** Use reproducible `content_hash` in upsert as ID.
  * **Insert in Vector DB passes only last chunk meta\_data:** Insert in vector db passes only last chunk meta\_data. issue link- [https://discord.com/channels/965734768803192842/1219054452221153463/1376631140047130649](https://discord.com/channels/965734768803192842/1219054452221153463/1376631140047130649)
  * **Remove Argument Sanitization:** Replaced with a safer way to do this that won't break arguments that shouldn't be sanitized
  * **Handle async tools when running async agents on playground:** Fixed a regression where using Agents with async tools (e.g. MCP tools) was breaking in the Playground.
</Update>

<Update label="2025-05-23" description="v1.5.4">
  ## New Features:

  * **User Control Flows**: This is the beta release of Agno’s Human-in-the-loop flows and tools.
    * We now allow agent runs to be `paused` awaiting completion of certain user requirements before the agent can continue.
    * This also adds the `agent.continue_run` and `agent.acontinue_run` functions.
    * The control flows that are available:
      * User confirmation flow → Decorate a function with `@tool(requires_confirmation=True)` and the agent will expect user confirmation before executing the tool.
      * User input required → Decorate a function with `@tool(requires_user_input=True)` to have the agent stop and ask for user input before continuing.
      * External tool execution → Decorate a function with `@tool(external_execution=True)` to indicate that you will execute this function outside of the agent context.
      * Dynamic user input → Add `UserControlFlowTools()` to an agent to give the agent the ability to dynamically stop the flow and ask for user input where required.
    * See a host of examples [here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/user_control_flows).
  * **Mem0 Toolkit**: Added a toolkit for managing memories in Mem0.
  * **Firecrawl Search**: Added support for Firecrawl web search in `FirecrawlTools`.

  ## Bug Fixes:

  * **Firecrawl Tools and Reader**: Fixed parameter parsing for the Firecrawl reader and tools.
  * **Include/Exclude on all Tools**: Ensure all toolkits support `include_tools` and `exclude_tools`.
</Update>

<Update label="2025-05-21" description="v1.5.3">
  ## Improvements:

  * **Improved Accuracy Evals:** Updated the way accuracy evals works for more accurate agent-based evaluation.

  ## Bug Fixes:

  * **MCP Client Timeout:** Client timeouts now work correctly and use the timeout set on parameters.
</Update>

<Update label="2025-05-20" description="v1.5.2">
  ## New Features:

  * **Agno Apps (Beta)**: Introducing Agno Apps, convenience functions to assist with building production-ready applications. The first supported apps are:
    * `FastAPIApp` → A really simple FastAPI server that provides access to your `agent` or `team`.
    * `WhatsappAPIApp` → An app that implements the Whatsapp protocol allowing you to have an Agno agent running on Whatsapp. Supports image / audio / video input and can generate images as responses. Supports reasoning.
  * **Couchbase Vector DB Support**: Added support for Couchbase as a vector DB for knowledge bases.
  * **Knowledge Filters Update for Teams:** Filters (manual + agentic) can now be used with Teams.
  * **Azure Cosmos DB for MongoDB (vCore) vector db support:**  In the MongoDB vector db class add support for cosmosdb mongo vcore support by enabling `cosmos_compatibility=True`
  * **Google Big Query Tools**: Added Toolkit for Google BigQuery support.
  * **Async Support for s3 Readers:** Add async support for `pdf` and `text` s3 readers.
  * **`stop_after_tool_call` and `show_result` for Toolkits:** Now the base Toolkit class has `stop_after_tool_call_tools` and `show_result_tools` similar to the `@tool` decorator.
</Update>

<Update label="2025-05-16" description="v1.5.1">
  ## New Features:

  * **Nebius Model Provider**: Added [Nebius](https://studio.nebius.com/) as a model provider.
  * **Extended Filters Support on Vector DBs**: Added filtering support for other vector DBs.
    * pgvector
    * Milvus
    * Weaviate
    * Chroma

  ## Improvements:

  * **Redis SSL**: Added the `ssl` parameter to `Redis` storage.
</Update>

<Update label="2025-05-13" description="v1.5.0">
  ## New Features:

  * **Azure OpenAI Tools**: Added image generation via Dall-E via Azure AI Foundry.
  * **OpenTelemetry Instrumentation:** We have contributed to the [OpenInference](https://github.com/Arize-ai/openinference) project and added an auto-instrumentor for Agno agents. This adds tracing instrumentation for Agno Agents for any OpenTelemetry-compatible observability provider. These include Arize, Langfuse and Langsmith. Examples added to illustrate how to use each one ([here](https://github.com/agno-agi/agno/tree/main/cookbook/observability)).
  * **Evals Updates**: Added logic to run accuracy evaluations with pre-generated answers and minor improvements for all evals classes.
  * **Hybrid Search and Reranker for Milvus Vector DB:** Added support for `hybrid_search` on Milvus.
  * **MCP with Streamable-HTTP:** Now supporting the streamable-HTTP transport for MCP servers.

  ## Improvements:

  * **Knowledge Filters Cookbook:** Instead of storing the sample data locally, we now pull it from s3 at runtime to keep the forking of the repo as light as possible.

  ## Bug Fixes:

  * **Team Model State:** Fixed issues related to state being shared between models on teams.
  * **Concurrent Agent Runs**: Fixed certain race-conditions related to running agents concurrently.

  ## Breaking changes:

  * **Evals Refactoring:**
    * Our performance evaluation class has been renamed from `PerfEval` to `PerformanceEval`
    * Our accuracy evaluation class has new required fields: `agent`, `prompt` and `expected_answer`
  * **Concurrent Agent Runs:** We removed duplicate information from some events during streaming (`stream=True`). Individual events will have more relevant data now.
</Update>

<Update label="2025-05-10" description="v1.4.6">
  ## New Features:

  * **Cerebras Model Provider**: Added Cerebras as a model provider.
  * **Claude Web Search**: Added support for [Claude’s new web search tool](https://www.anthropic.com/news/web-search).
  * **Knowledge Base Metadata Filtering (Beta)**: Added support for filtering documents by metadata
    * **Two Ways to Apply Filters**:
      * **Explicit Filtering**: Pass filters directly to Agent or during run/query

        ```python
        # Option 1: Filters on Agent initialization
        agent = Agent(
        					knowledge=knowledge_base, 
        					knowledge_filters={"filter_1": "abc"}
        				)
             
        # Option 2: Filters on run execution
        agent.run("Tell me about...", knowledge_filters={"filter_1": "abc"})
        ```

        See docs [here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/filters/pdf/filtering.py)

      * **Agentic Filtering**: Agent automatically detects and applies filters from user queries

        ```python
        # Enable automatic filter detection
        agent = Agent(
        					knowledge=knowledge_base, 
        					enable_agentic_knowledge_filters=True
        				)
             
        # Agent extracts filters from query
        agent.run("Tell me about John Doe's experience...")
        ```

        See docs [here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/filters/pdf/agentic_filtering.py)
    * Two approaches for adding metadata to documents:
      1. **During Knowledge Base Initialization**:

         ```python
         knowledge_base = PDFKnowledgeBase(path=[
              {
         		     "path": "file_1.pdf", 
         		     "metadata": {
         				     "user_id": "abc"
         				  }
         		 },
         		 {
         		     "path": "file_2.pdf", 
         		     "metadata": {
         				     "user_id": "xyz"
         				  }
         		 }
         ])
         ```

      2. **During Individual Document Loading:**

         ```python
         knowledge_base.load_document(
              path="file.pdf",
              metadata={"user_id": "abc"}
         )
         ```
    * **Compatibility**
      * **Knowledge Base Types**: `PDF`, `Text`, `DOCX`, `JSON`, and `PDF_URL`
      * **Vector Databases**: `Qdrant`, `LanceDB`, and `MongoDB`

  ## Improvements:

  * **User and Session ID in Tools**: Added `current_user_id` and `current_session_id` as default variables in `session_data` for `Agent` and `Team`.

  ## Bug Fixes:

  * **Knowledge Base ID Clashes**: Knowledge files with overlapping names (e.g., `abc.-.xyz.pdf` and `abc.-.def.pdf`) were being incorrectly identified due to the readers using formatted names as unique id which were getting uniqueness conflict. Introduced a unique ID for each document in all the readers using `uuidv4()` to ensure strict identification and prevent conflicts.
</Update>

<Update label="2025-05-06" description="v1.4.5">
  ## New Features:

  * **Embedder Support via AWS Bedrock**: `AwsBedrockEmbedder` has been added with a default embedding model of `cohere.embed-multilingual-v3`.
  * **Gemini Video Generation Tool**: Added video generation capabilities to `GeminiTools`.

  ## Improvements:

  * **Apify Revamp**: Complete revamp of `ApifyTools` to make it completely compatible with Apify actors.

  ## Bug Fixes:

  * **Tools with Optional Parameters on Llama API**: Fixed edge cases with functions.
</Update>

<Update label="2025-05-03" description="v1.4.4">
  ## New Features:

  * **OpenAI File Support:** Added support for `File` attached to prompts for agents with `OpenAIChat` models.

  ## Improvements:

  * **Llama API:** Various improvements for Llama and LlamaOpenAI model classes including structured output and image input support
  * **Async Custom Retriever**: The `retriever` parameter can now be an `async` function to be used with `agent.arun` and `agent.aprint_response`.
  * **Gemini Video URL Input**: Added support for `Video(url=...)` for Gemini.

  ## Bug Fixes:

  * **OpenAI Responses o3 / o4 Tools**: Fixed broken tool use for advanced reasoning models on `OpenAIResponses`.
  * **MCP on CLI Support**: Fixed support for `MCPTools` usage while calling `agent.acli_app`.
</Update>

<Update label="2025-04-30" description="v1.4.3">
  ## **New Features:**

  * **Llama API:** Added native SDK and OpenAI-like model classes.

  ## **Improvements:**

  * **Claude**: Added support for AWS Session token for Claude.
  * **DynamoDB**: Added support for AWS profile-based authentication.

  ## **Bug Fixes:**

  * **Session Metrics**: Fix for session metrics showing up as 0.
  * **HF Embedder fix**: Fixed Hugging Face Embedder.
</Update>

<Update label="2025-04-25" description="v1.4.2">
  ## New Features:

  * **MCP SSE Support**: Added support for connecting to SSE MCP Servers.
  * **Tool Hooks**: You can now have a hook that is wrapped around all tool calls. This works for `Toolkits` and custom tools. See [this example](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/tool_concepts/toolkits/tool_hook.py).
  * **Team Session State:** You can now manage a single state dictionary across a team leader and team members inside tools given to the team leader/members. See [this example](https://github.com/agno-agi/agno/blob/main/cookbook/teams/team_with_shared_state.py).
  * **Cartesia Tool**: Added support for Cartesia for text-to-speech capabilities.
  * **Gemini Image Tools:** Added a tool that uses Gemini models to generate images.
  * **Groq Audio Tools**: Added a tool that uses Groq models to translate, transcribe and generate audio.

  ## Improvements:

  * **PubmedTools Expanded Results**: Added expanded result sets for `PubmedTools` .
  * **Variety in Tool Results**: Custom tools can now have any return type and it would be handled before being provided to the model.

  ## Bug Fixes:

  * **Teams Shared Model Bug**: Fixed issues where a single model is used across team members. This should reduce tool call failures in team execution.
</Update>

<Update label="2025-04-23" description="v1.4.0">
  ## New Features:

  * **Memory Generally Available**: We have made improvements and adjustments to how Agentic user memory management works. This is now out of beta and generally available. See these [examples](https://github.com/agno-agi/agno/tree/main/cookbook/agent_concepts/memory) and these [docs](https://docs.agno.com/agents/memory) for more info.
  * **OpenAI Tools**: Added `OpenAITools` to enable text-to-speech and image generation through OpenAI’s APIs.
  * **Zep Tools**: Added `ZepTools` and `AsyncZepTools` to manage memories for your Agent using `zep-cloud`

  ## Improvements:

  * **Azure AI Foundry Reasoning**: Added support for reasoning models via Azure AI Foundry. E.g. Deepseek-R1.
  * **Include/Exclude Tools**: Added `include_tools` and `exclude_tools` for all toolkits. This allows for selective enabling / disabling of tools inside toolkits, which is especially useful for larger toolkits.

  ## Bug Fixes:

  * **Gemini with Memory**: Fixed issue with `deepcopy` when Gemini is used with `Memory`.

  ## Breaking Changes:

  * **Memory:** Agents will now by default use an improved `Memory` instead of the now deprecated `AgentMemory`. - `agent.memory.messages` → `run.messages for run in agent.memory.runs` (or `agent.get_messages_for_session()`) - `create_user_memories` → `enable_user_memories` and is now set on the Agent/Team directly. - `create_session_summary` → `enable_session_summaries` and is now set on the Agent/Team directly.
</Update>

<Update label="2025-04-21" description="v1.3.5">
  ## Improvements:

  * **Further Async Vector DB Support**: Support added for:
    * [Clickhouse](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/clickhouse_db/async_clickhouse.py)
    * [ChromaDB](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/chroma_db/async_chroma_db.py)
    * [Cassandra](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/cassandra_db/async_cassandra_db.py)
    * [PineconeDB](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/pinecone_db/async_pinecone_db.py)
    * [Pgvector](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/pgvector_db/async_pg_vector.py)
  * **Reasoning on Agno Platform**:
    * Added extensive support for reasoning on the Agno Platform. Go see your favourite reasoning agents in action!
    * Changes from SDK
      * send proper events in different types of reasoning and populate the `reasoning_content` on `RunResponse` for `stream/non-stream`, `async/non-async`
      * unified json structure for all types of reasoning in `Reasoning events`
  * **Google Caching Support**: Added support for caching files and sending the cached content to Gemini.

  ## Bug Fixes

  * **Firecrawl Scrape**: Fixed issues with non-serializable types for during Firecrawl execution. [https://github.com/agno-agi/agno/issues/2883](https://github.com/agno-agi/agno/issues/2883)
</Update>

<Update label="2025-04-18" description="v1.3.4">
  ## New Features:

  * **Web Browser Tool:** Introduced a `webbrowser` tool for agents to interact with the web.
  * **Proxy Support:** Added `proxy` parameter support to both URL and PDF tools for network customization.

  ## Improvements:

  * **Session State:** Added examples for managing session state in agents.
  * **AzureOpenAIEmbedder:** Now considers `client_params` passed in the `client_params` argument for more flexible configuration.
  * **LiteLLM:** Now uses built-in environment validation to simplify setup.
  * **Team Class:** Added a `mode` attribute to team data serialization for enhanced team configuration.
  * **Insert/Upsert/Log Optimization:** insert/upsert/log\_info operations now trigger only when documents are present in the reader.
  * **Database Preference:** Session state now prefers database-backed storage if available.
  * **Memory Management:** Internal memory system updated for better session handling and resource efficiency.
  * **Module Exports:** Init files that only import now explicitly export symbols using `__all__`.

  ## Bug Fixes:

  * **DynamoDB Storage:** Fixed an issue with storage handling in DynamoDB-based setups.
  * **DeepSeek:** Fixed a bug with API key validation logic.
</Update>

<Update label="2025-04-17" description="v1.3.3">
  ## Improvements:

  * **Gemini File Upload**: Enabled direct use of uploaded files with Gemini.
  * **Metrics Update**: Added audio, reasoning and cached token counts to metrics where available on models.
  * **Reasoning Updates**: We now natively support Ollama and AzureOpenAI reasoning models.

  ## Bug Fixes:

  * **PPrint Util Async**: Added `apprint_run_response` to support async.
  * **Mistral Reasoning:** Fixed issues with using a Mistral model for chain-of-thought reasoning.
</Update>

<Update label="2025-04-16" description="v1.3.2">
  ## New Features:

  * **Redis Memory DB**: Added Redis as a storage provider for `Memory`. See [here](https://docs.agno.com/examples/concepts/memory/mem-redis-memory).

  ## Improvements:

  * **Memory Updates**: Various performance improvements made and convenience functions added:
    * `agent.get_session_summary()` → Use to get the previous session summary from the agent.
    * `agent.get_user_memories()` → Use to get the current user’s memories.
    * You can also add additional instructions to the `MemoryManager` or `SessionSummarizer`.
  * **Confluence Bypass SSL Verification**: If required, you can now skip SSL verification for Confluence connections.
  * **More Flexibility On Team Prompts**: Added `add_member_tools_to_system_message` to remove the member tool names from the system message given to the team leader, which allows flexibility to make teams transfer functions work in more cases.

  ## Bug Fixes:

  * **LiteLLM Streaming Tool Calls**: Fixed issues with tool call streaming in LiteLLM.
  * **E2B Casing Issue**: Fixed issues with parsed Python code that would make some values lowercase.
  * **Team Member IDs**: Fixed edge-cases with team member IDs causing teams to break.
</Update>

<Update label="2025-04-12" description="v1.3.0">
  ## New Features:

  * **Memory Revamp (Beta)**: This is a beta release of a complete revamp of Agno Memory. This includes a new `Memory` class that supports adding, updating and deleting user memories, as well as doing semantic search with a model. This also adds additional abilities to the agent to manage memories on your behalf. See the docs [here](https://docs.agno.com/memory/introduction).
  * **User ID and Session ID on Run**: You can now pass `user_id` and `session_id` on `agent.run()`. This will ensure the agent is set up for the session belonging to the `session_id` and that only the memories of the current user is accessible to the agent. This allows you to build multi-user and multi-session applications with a single agent configuration.
  * **Redis Storage**: Support added for Redis as a session storage provider.
</Update>

<Update label="2025-04-11" description="v1.2.16">
  ## Improvements:

  * **Teams Improvements**: Multiple improvements to teams to make task forwarding to member agents more reliable and to make the team leader more conversational. Also added various examples of reasoning with teams.
  * **Knowledge on Teams**: Added `knowledge` to `Team` to better align with the functionality on `Agent`. This comes with `retriever` to set a custom retriever and `search_knowledge` to enable Agentic RAG.

  ## Bug Fixes:

  * **Gemini Grounding Chunks**: Fixed error when Gemini Grounding was used in streaming.
  * **OpenAI Defaults in Structured Outputs**: OpenAI does not allow defaults in structured outputs. To make our structured outputs as compatible as possible without adverse effects, we made updates to `OpenAIResponses` and `OpenAIChat`.
</Update>

<Update label="2025-04-08" description="v1.2.14">
  ## Improvements:

  * **Improved Github Tools**: Added many more capabilities to `GithubTools`.
  * **Windows Scripts Support**: Converted all the utility scripts to be Windows compatible.
  * **MongoDB VectorDB Async Support**: MongoDB can now be used in async knowledge bases.

  ## Bug Fixes:

  * **Gemini Tool Formatting**: Fixed various cases where functions would not be parsed correctly when used with Gemini.
  * **ChromaDB Version Compatibility:** Fix to ensure that ChromaDB and Agno are compatible with newer versions of ChromaDB.
  * **Team-Member Interactions**: Fixed issue where if members respond with empty content the team would halt. This is now be resolved.
  * **Claude Empty Response:** Fixed a case when the response did not include any content with tool calls resulting in an error from the Anthropic API
</Update>

<Update label="2025-04-07" description="v1.2.12">
  ## New Features:

  * **Timezone Identifier:** Added a new `timezone_identifier` parameter in the Agent class to include the timezone alongside the current date in the instructions.
  * **Google Cloud JSON Storage**: Added support for JSON-based session storage on Google Cloud.
  * **Reasoning Tools**: Added `ReasoningTools` for an advanced reasoning scratchpad for agents.

  ## Improvements:

  * **Async Vector DB and Knowledge Base Improvements**: More knowledge bases have been updated for `async-await` support: - `URLKnowledgeBase` → Find some examples [here](https://github.com/agno-agi/agno/blob/9d1b14af9709dde1e3bf36c241c80fb295c3b6d3/cookbook/agent_concepts/knowledge/url_kb_async.py). - `FireCrawlKnowledgeBase` → Find some examples [here](https://github.com/agno-agi/agno/blob/596898d5ba27d2fe228ea4f79edbe9068d34a1f8/cookbook/agent_concepts/knowledge/firecrawl_kb_async.py). - `DocxKnowledgeBase` → Find some examples [here](https://github.com/agno-agi/agno/blob/f6db19f4684f6ab74044a4466946e281586ca1cf/cookbook/agent_concepts/knowledge/docx_kb_async.py).
</Update>

<Update label="2025-04-07" description="v1.2.11">
  ## Bug Fixes:

  * **Fix for structured outputs**: Fixed cases of structured outputs for reasoning.
</Update>

<Update label="2025-04-07" description="v1.2.10">
  ## 1.2.10

  ## New Features:

  * **Knowledge Tools**: Added `KnowledgeTools` for thinking, searching and analysing documents in a knowledge base.
</Update>

<Update label="2025-04-05" description="v1.2.9">
  ## 1.2.9

  ## Improvements:

  * **Simpler MCP Interface**: Added `MultiMCPTools` to support multiple server connections and simplified the interface to allow `command` to be passed. See [these examples](https://github.com/agno-agi/agno/blob/382667097c31fbb9f08783431dcac5eccd64b84a/cookbook/tools/mcp) of how to use it.
</Update>

<Update label="2025-04-04" description="v1.2.8">
  ## 1.2.8

  # Changelog

  ## New Features:

  * **Toolkit Instructions**: Extended `Toolkit` with `instructions` and `add_instructions` to enable you to specify additional instructions related to how a tool should be used. These instructions are then added to the model’s “system message” if `add_instructions=True` .

  ## Bug Fixes:

  * **Teams transfer functions**: Some tool definitions of teams failed for certain models. This has been fixed.
</Update>

<Update label="2025-04-02" description="v1.2.7">
  ## 1.2.7

  ## New Features:

  * **Gemini Image Generation**: Added support for generating images straight from Gemini using the `gemini-2.0-flash-exp-image-generation` model.

  ## Improvements:

  * **Vertex AI**: Improved use of Vertex AI with Gemini Model class to closely follow the official Google specification
  * **Function Result Caching Improvement:** We now have result caching on all Agno Toolkits and any custom functions using the `@tool` decorator. See the docs [here](https://docs.agno.com/tools/functions).
  * **Async Vector DB and Knowledge Base Improvements**: Various knowledge bases, readers and vector DBs now have `async-await` support, so it will be used in `agent.arun` and `agent.aprint_response`. This also means that `knowledge_base.aload()` is possible which should greatly increase loading speed in some cases. The following have been converted:
    * Vector DBs:
      * `LanceDb` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/lance_db/async_lance_db.py) is a cookbook to illustrate how to use it.
      * `Milvus` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/milvus_db/async_milvus_db.py) is a cookbook to illustrate how to use it.
      * `Weaviate` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/vector_dbs/weaviate_db/async_weaviate_db.py) is a cookbook to illustrate how to use it.
    * Knowledge Bases:
      * `JSONKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/json_kb_async.py) is a cookbook to illustrate how to use it.
      * `PDFKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/pdf_kb_async.py) is a cookbook to illustrate how to use it.
      * `PDFUrlKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/pdf_url_kb_async.py) is a cookbook to illustrate how to use it.
      * `CSVKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/csv_kb_async.py) is a cookbook to illustrate how to use it.
      * `CSVUrlKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/csv_url_kb_async.py) is a cookbook to illustrate how to use it.
      * `ArxivKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/arxiv_kb_async.py) is a cookbook to illustrate how to use it.
      * `WebsiteKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/website_kb_async.py) is a cookbook to illustrate how to use it.
      * `YoutubeKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/youtube_kb_async.py) is a cookbook to illustrate how to use it.
      * `TextKnowledgeBase` → [Here](https://github.com/agno-agi/agno/blob/main/cookbook/agent_concepts/knowledge/text_kb_async.py) is a cookbook to illustrate how to use it.

  ## Bug Fixes:

  * **Recursive Chunking Infinite Loop**: Fixes an issue with RecursiveChunking getting stuck in an infinite loop for large documents.
</Update>

<Update label="2025-03-28" description="v1.2.6">
  ## 1.2.6

  ## Bug Fixes:

  * **Gemini Function call result fix**: Fixed a bug with function call results failing formatting and added proper role mapping .
  * **Reasoning fix**: Fixed an issue with default reasoning and improved logging for reasoning models .
</Update>

<Update label="2025-03-27" description="v1.2.5">
  ## 1.2.5

  ## New Features:

  * **E2B Tools:** Added E2B Tools to run code in E2B Sandbox

  ## Improvements:

  * **Teams Tools**: Add `tools` and `tool_call_limit` to `Team`. This means the team leader itself can also have tools provided by the user, so it can act as an agent.
  * **Teams Instructions:** Improved instructions around attached images, audio, videos, and files. This should increase success when attaching artifacts to prompts meant for member agents.
  * **MCP Include/Exclude Tools**: Expanded `MCPTools` to allow you to specify tools to specifically include or exclude from all the available tools on an MCP server. This is very useful for limiting which tools the model has access to.
  * **Tool Decorator Async Support**: The `@tool()` decorator now supports async functions, including async pre and post-hooks.

  ## Bug Fixes:

  * **Default Chain-of-Thought Reasoning:** Fixed issue where reasoning would not default to manual CoT if the provided reasoning model was not capable of reasoning.
  * **Teams non-markdown responses**: Fixed issue with non-markdown responses in teams.
  * **Ollama tool choice:** Removed `tool_choice` from Ollama usage as it is not supported.
  * **Worklow session retrieval from storage**: Fixed `entity_id` mappings.
</Update>

<Update label="2025-03-25" description="v1.2.4">
  ## 1.2.4

  ## Improvements:

  * **Tool Choice on Teams**: Made `tool_choice` configurable.

  ## Bug Fixes:

  * **Sessions not created**: Made issue where sessions would not be created in existing tables without a migration be more visible. Please read the docs on [storage schema migrations](https://docs.agno.com/agents/storage).
  * **Todoist fixes**: Fixed `update_task` on `TodoistTools`.
</Update>

<Update label="2025-03-24" description="v1.2.3">
  ## 1.2.3

  ## Improvements:

  * **Teams Error Handling:** Improved the flow in cases where the model gets it wrong when forwarding tasks to members.
</Update>

<Update label="2025-03-24" description="v1.2.2">
  ## 1.2.2

  ## Bug Fixes:

  * **Teams Memory:** Fixed issues related to memory not persisting correctly across multiple sessions.
</Update>

<Update label="2025-03-24" description="v1.2.1">
  ## 1.2.1

  ## Bug Fixes:

  * **Teams Markdown**: Fixed issue with markdown in teams responses.
</Update>

<Update label="2025-03-24" description="v1.2.0">
  ## 1.2.0

  ## New Features:

  * **Financial Datasets Tools**: Added tools for [https://www.financialdatasets.ai/](https://www.financialdatasets.ai/).
  * **Docker Tools**: Added tools to manage local docker environments.

  ## Improvements:

  * **Teams Improvements:** Reasoning enabled for the team.
  * **MCP Simplification:** Simplified creation of `MCPTools` for connections to external MCP servers. See the updated [docs](https://docs.agno.com/tools/mcp#example%3A-filesystem-agent).

  ## Bug Fixes:

  * **Azure AI Factory:** Fix for a broken import in Azure AI Factory.
</Update>

<Update label="2025-03-23" description="v1.1.17">
  ## 1.1.17

  ## Improvements:

  * **Better Debug Logs**: Enhanced debug logs for better readability and clarity.
</Update>

<Update label="2025-03-22" description="v1.1.16">
  ## 1.1.16

  ## New Features:

  * **Async Qdrant VectorDB:** Implemented async support for Qdrant VectorDB, improving performance and efficiency.
  * **Claude Think Tool:** Introduced the Claude **Think tool**, following the specified implementation [guide.](https://www.anthropic.com/engineering/claude-think-tool)
</Update>

<Update label="2025-03-21" description="v1.1.15">
  ## 1.1.15

  ## Improvements:

  * **Tool Result Caching:** Added caching of selected searchers and scrapers. This is only intended for testing and should greatly improve iteration speed, prevent rate limits and reduce costs (where applicable) when testing agents. Applies to:
    * DuckDuckGoTools
    * ExaTools
    * FirecrawlTools
    * GoogleSearchtools
    * HackernewsTools
    * NewspaperTools
    * Newspaper4kTools
    * Websitetools
    * YFinanceTools
  * **Show tool calls**: Improved how tool calls are displayed when `print_response` and `aprint_response` is used. They are now displayed in a separate panel different from response panel. It can also be used in conjunction in `response_model`.
</Update>

<Update label="2025-03-20" description="v1.1.14">
  ## 1.1.14 - Teams Revamp

  ## New Features:

  * **Teams Revamp**: Announcing a new iteration of Agent teams with the following features:
    * Create a `Team` in one of 3 modes: “Collaborate”, “Coordinate” or “Route”.
    * Various improvements have been made that was broken with the previous teams implementation. Including returning structured output from member agents (for “route” mode), passing images, audio and video to member agents, etc.
    * It has added features like “agentic shared context” between team members and sharing of individual team member responses with other team members.
    * This also comes with a revamp of Agent and Team debug logs. Use `debug_mode=True` and `team.print_response(...)` to see it in action.
    * Find the docs [here](https://docs.agno.com/teams/introduction). Please look at the example implementations [here](https://github.com/agno-agi/agno/blob/c8e47d1643065a0a6ee795c6b063f8576a7a2ef6/cookbook/examples/teams).
    * This is the first release. Please give us feedback. Updates and improvements will follow.
    * Support for `Agent(team=[])` is still there, but deprecated (see below).
  * **LiteLLM:** Added [LiteLLM](https://www.litellm.ai/) support, both as a native implementation and via the `OpenAILike` interface.

  ## Improvements:

  * **Change structured\_output to response\_format:** Added `use_json_mode: bool = False` as a parameter of `Agent` and `Team`, which in conjunction with `response_model=YourModel`, is used to indicate whether the agent/team model should be forced to respond in json instead of (now default) structured output. Previous behaviour defaulted to “json-mode”, but since most models now support native structured output, we are now defaulting to native structured output. It is now also much simpler to work with response models, since now only `response_model` needs to be set. It is not necessary anymore to set `structured_output=True` to specifically get structured output from the model.
  * **Website Tools + Combined Knowledgebase:** Added functionality for `WebsiteTools` to also update combined knowledgebases.

  ## Bug Fixes:

  * **AgentMemory**: Fixed `get_message_pairs()` fetching incorrect messages.
  * **UnionType in Functions**: Fixed issue with function parsing where pipe-style unions were used in function parameters.
  * **Gemini Array Function Parsing**: Fixed issue preventing gemini function parsing to work in some MCP cases.

  ## Deprecations:

  * **Structured Output:** `Agent.structured_output` has been replaced by `Agent.use_json_mode`. This will be removed in a future major version release.
  * **Agent Team:** `Agent.team` is deprecated with the release of our new Teams implementation [here](https://docs.agno.com/teams/introduction). This will be removed in a future major version release.
</Update>

<Update label="2025-03-14" description="v1.1.13">
  ## 1.1.13

  ## Improvements:

  * **OpenAIResponses File Search**: Added support for the built-in [“File Search”](https://platform.openai.com/docs/guides/tools-file-search) function from OpenAI. This automatically uploads `File` objects attached to the agent prompt.
  * **OpenAIReponses web citations**: Added support to extract URL citations after usage of the built-in “Web Search” tool from OpenAI.
  * **Anthropic document citations**: Added support to extract document citations from Claude responses when `File` objects are attached to agent prompts.
  * **Cohere Command A**: Support and examples added for Coheres new flagship model

  ## Bug Fixes:

  * **Ollama tools**: Fixed issues with tools where parameters are not typed.
  * **Anthropic Structured Output**: Fixed issue affecting Anthropic and Anthropic via Azure where structured output wouldn’t work in some cases. This should make the experience of using structured output for models that don’t natively support it better overall. Also now works with enums as types in the Pydantic model.
  * **Google Maps Places**: Support from Google for Places API has been changed and this brings it up to date so we can continue to support “search places”.
</Update>

<Update label="2025-03-13" description="v1.1.12">
  ## 1.1.12

  ## New Features:

  * **Citations**: Improved support for capturing, displaying, and storing citations from models, with integration for Gemini and Perplexity.

  ## Improvements:

  * **CalComTools**: Improvement to tool Initialization.

  ## Bug Fixes:

  * **MemoryManager**: Limit parameter was added fixing a KeyError in MongoMemoryDb.
</Update>

<Update label="2025-03-13" description="v1.1.11">
  ## 1.1.11

  ## New Features:

  * **OpenAI Responses**: Added a new model implementation that supports OpenAI’s Responses API. This includes support for their [“websearch”](https://platform.openai.com/docs/guides/tools-web-search#page-top) built-in tool.
  * **Openweather API Tool:** Added tool to get real-time weather information.

  ## Improvements:

  * **Storage Refactor:** Merged agent and workflow storage classes to align storage better for agents, teams and workflows. This change is backwards compatible and should not result in any disruptions.
</Update>

<Update label="2025-03-12" description="v1.1.10">
  ## 1.1.10

  ## New Features:

  * **File Prompts**: Introduced a new `File` type that can be added to prompts and will be sent to the model providers. Only Gemini and Anthropic Claude supported for now.
  * **LMStudio:** Added support for [LMStudio](https://lmstudio.ai/) as a model provider. See the [docs](https://docs.agno.com/models/lmstudio).
  * **AgentQL Tools**: Added tools to support [AgentQL](https://www.agentql.com/) for connecting agents to websites for scraping, etc. See the [docs](https://docs.agno.com/tools/toolkits/agentql).
  * **Browserbase Tool:** Added [Browserbase](https://www.browserbase.com/) tool.

  ## Improvements:

  * **Cohere Vision**: Added support for image understanding with Cohere models. See [this cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/models/cohere/image_agent.py) to try it out.
  * **Embedder defaults logging**: Improved logging when using the default OpenAI embedder.

  ## Bug Fixes:

  * **Ollama Embedder**: Fix for getting embeddings from Ollama across different versions.
</Update>

<Update label="2025-03-06" description="v1.1.9">
  ## 1.1.9

  ## New Features:

  * **IBM Watson X:** Added support for IBM Watson X as a model provider. Find the docs [here](https://docs.agno.com/models/ibm-watsonx).
  * **DeepInfra**: Added support for [DeepInfra](https://deepinfra.com). Find the docs [here](https://docs.agno.com/models/deepinfra).
  * **Support for MCP**: Introducing `MCPTools` along with examples for using MCP with Agno agents.

  ## Bug Fixes:

  * **Mistral with reasoning**: Fixed cases where Mistral would fail when reasoning models from other providers generated reasoning content.
</Update>

<Update label="2025-03-03" description="v1.1.8">
  ## 1.1.8

  ## New Features:

  * **Video File Upload on Playground**: You can now upload video files and have a model interpret the video. This feature is supported only by select `Gemini` models with video processing capabilities.

  ## Bug Fixes:

  * **Huggingface**: Fixed multiple issues with the `Huggingface` model integration. Tool calling is now fully supported in non-streaming cases.
  * **Gemini**: Resolved an issue with manually setting the assistant role and tool call result metrics.
  * **OllamaEmbedder**: Fixed issue where no embeddings were returned.
</Update>

<Update label="2025-02-26" description="v1.1.7">
  ## 1.1.7

  ## New Features:

  * **Audio File Upload on Playground**: You can now upload audio files and have a model interpret the audio, do sentiment analysis, provide an audio transcription, etc.

  ## Bug Fixes:

  * **Claude Thinking Streaming**: Fix Claude thinking when streaming is active, as well as for async runs.
</Update>

<Update label="2025-02-24" description="v1.1.6">
  ## 1.1.6

  ## New Features:

  -**Claude 3.7 Support:** Added support for the latest Claude 3.7 Sonnet model

  ## Bug Fixes:

  -**Claude Tool Use**: Fixed an issue where tools and content could not be used in the same block when interacting with Claude models.
</Update>

<Update label="2025-02-24" description="v1.1.5">
  ## 1.1.5

  ## New Features:

  * **Audio Responses:** Agents can now deliver audio responses (both with streaming and non-streaming).

    * The audio is in the `agent.run_response.response_audio`.

    * This only works with `OpenAIChat` with the `gpt-4o-audio-preview` model. See [their docs](https://platform.openai.com/docs/guides/audio) for more on how it works. For example

      ```python
      from agno.agent import Agent
      from agno.models.openai import OpenAIChat
      from agno.utils.audio import write_audio_to_file

      agent = Agent(
          model=OpenAIChat(
              id="gpt-4o-audio-preview",
              modalities=["text", "audio"],  # Both text and audio responses are provided.
              audio={"voice": "alloy", "format": "wav"},
          ),
      )
      agent.print_response(
          "Tell me a 5 second story"
      )
      if agent.run_response.response_audio is not None:
          write_audio_to_file(
              audio=agent.run_response.response_audio.base64_audio, filename=str(filename)
          )
      ```

    * See the [audio\_conversation\_agent cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/playground/audio_conversation_agent.py) to test it out on the Agent Playground.

  * **Image understanding support for [Together.ai](http://Together.ai) and XAi**: You can now give images to agents using models from XAi and Together.ai.

  ## Improvements:

  * **Automated Tests:** Added integration tests for all models. Most of these will be run on each pull request, with a suite of integration tests run before a new release is published.
  * **Grounding and Search with Gemini:** [Grounding and Search](https://ai.google.dev/gemini-api/docs/grounding?lang=python) can be used to improve the accuracy and recency of responses from the Gemini models.

  ## Bug Fixes:

  * **Structured output updates**: Fixed various cases where native structured output was not used on models.
  * **Ollama tool parsing**: Fixed cases for Ollama with tools with optional parameters.
  * **Gemini Memory Summariser**: Fixed cases where Gemini models were used as the memory summariser.
  * **Gemini auto tool calling**: Enabled automatic tool calling when tools are provided, aligning behavior with other models.
  * **FixedSizeChunking issue with overlap:** Fixed issue where chunking would fail if overlap was set.
  * **Claude tools with multiple types**: Fixed an issue where Claude tools would break when handling a union of types in parameters.
  * **JSON response parsing**: Fixed cases where JSON model responses returned quoted strings within dictionary values.
</Update>

<Update label="2025-02-17" description="v1.1.4">
  ## 1.1.4

  ## Improvements:

  * **Gmail Tools**: Added `get_emails_by_thread` and `send_email_reply` methods to `GmailTools`.

  ## Bug Fixes:

  * **Gemini List Parameters**: Fixed an issue with functions using list-type parameters in Gemini.
  * **Gemini Safety Parameters**: Fixed an issue with passing safety parameters in Gemini.
  * **ChromaDB Multiple Docs:** Fixed an issue with loading multiple documents into ChromaDB.
  * **Agentic Chunking:** Fixed an issue where OpenAI was required for chunking even when a model was provided.
</Update>

<Update label="2025-02-16" description="v1.1.3">
  ## 1.1.3

  ## Bug Fixes:

  * **Gemini Tool-Call History**: Fixed an issue where Gemini rejected tool-calls from historic messages.
</Update>

<Update label="2025-02-15" description="v1.1.2">
  ## 1.1.2

  ## Improvements:

  * **Reasoning with o3 Models**: Reasoning support added for OpenAI’s o3 models.
  * **Gemini embedder update:** Updated the `GeminiEmbedder` to use the new [Google’s genai SDK](https://github.com/googleapis/python-genai). This update introduces a slight change in the interface:

    ```python
    # Before
    embeddings = GeminiEmbedder("models/text-embedding-004").get_embedding(
        "The quick brown fox jumps over the lazy dog."
    )

    # After
    embeddings = GeminiEmbedder("text-embedding-004").get_embedding(
        "The quick brown fox jumps over the lazy dog."
    )
    ```

  ## Bug Fixes:

  * **Singlestore Fix:** Fixed an issue where querying SingleStore caused the embeddings column to return in binary format.
  * **MongoDB Vectorstore Fix:** Fixed multiple issues in MongoDB, including duplicate creation and deletion of collections during initialization. All known issues have been resolved.
  * **LanceDB Fix:** Fixed various errors in LanceDB and added on\_bad\_vectors as a parameter.
</Update>

<Update label="2025-02-14" description="v1.1.1">
  ## 1.1.1

  ## Improvements:

  * **File / Image Uploads on Agent UI:** Agent UI now supports file and image uploads with prompts.
    * Supported file formats: `.pdf` , `.csv` , `.txt` , `.docx` , `.json`
    * Supported image formats: `.png` , `.jpeg` , `.jpg` , `.webp`
  * **Firecrawl Custom API URL**: Allowed users to set a custom API URL for Firecrawl.
  * **Updated `ModelsLabTools` Toolkit Constructor**: The constructor in `/libs/agno/tools/models_labs.py` has been updated to accommodate audio generation API calls. This is a breaking change, as the parameters for the `ModelsLabTools` class have changed. The `url` and `fetch_url` parameters have been removed, and API URLs are now decided based on the `file_type` provided by the user.

    ```python
    MODELS_LAB_URLS = {
        "MP4": "https://modelslab.com/api/v6/video/text2video",
        "MP3": "https://modelslab.com/api/v6/voice/music_gen",
        "GIF": "https://modelslab.com/api/v6/video/text2video",
    }

    MODELS_LAB_FETCH_URLS = {
        "MP4": "https://modelslab.com/api/v6/video/fetch",
        "MP3": "https://modelslab.com/api/v6/voice/fetch",
        "GIF": "https://modelslab.com/api/v6/video/fetch",
    }
    ```

    The `FileType` enum now includes `MP3` type:

    ```jsx
    class FileType(str, Enum):
        MP4 = "mp4"
        GIF = "gif"
        MP3 = "mp3"
    ```

  ## Bug Fixes:

  * **Gemini functions with no parameters:** Addressed an issue where Gemini would reject function declarations with empty properties.
  * **Fix exponential memory growth**: Fixed certain cases where the agent memory would grow exponentially.
  * **Chroma DB:** Fixed various issues related to metadata on insertion and search.
  * **Gemini Structured Output**: Fixed a bug where Gemini would not generate structured output correctly.
  * **MistralEmbedder:** Fixed issue with instantiation of `MistralEmbedder`.
  * **Reasoning**: Fixed an issue with setting reasoning models.
  * **Audio Response:** Fixed an issue with streaming audio artefacts to the playground.
</Update>

<Update label="2025-02-12" description="v1.1.0">
  ## 1.1.0 - Models Refactor and Cloud Support

  ## Model Improvements:

  * **Models Refactor**: A complete overhaul of our models implementation to improve on performance and to have better feature parity across models.
    * This improves metrics and visibility on the Agent UI as well.
    * All models now support async-await, with the exception of `AwsBedrock`.
  * **Azure AI Foundry**: We now support all models on Azure AI Foundry. Learn more [here](https://learn.microsoft.com/azure/ai-services/models)..
  * **AWS Bedrock Support**: Our redone AWS Bedrock implementation now supports all Bedrock models. It is important to note [which models support which features](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html).
  * **Gemini via Google SDK**: With the 1.0.0 release of [Google's genai SDK](https://github.com/googleapis/python-genai) we could improve our previous implementation of `Gemini`. This will allow for easier integration of Gemini features in future.
  * **Model Failure Retries:** We added better error handling of third-party errors (e.g. Rate-Limit errors) and the agent will now optionally retry with exponential backoff if `exponential_backoff` is set to `True`.

  ## Other Improvements

  * **Exa Answers Support**: Added support for the [Exa answers](https://docs.exa.ai/reference/answer) capability.
  * **GoogleSearchTools**: Updated the name of `GoogleSearch` to `GoogleSearchTools` for consistency.

  ## Deprecation

  * Our `Gemini` implementation directly on the Vertex API has been replaced by the Google SDK implementation of `Gemini`.
  * Our `Gemini` implementation via the OpenAI client has been replaced by the Google SDK implementation of `Gemini`.
  * Our `OllamaHermes` has been removed as the implementation of `Ollama` was improved.

  ## Bug Fixes

  * **Team Members Names**: Fixed a bug where teams where team members have non-aphanumeric characters in their names would cause exceptions.
</Update>

<Update label="2025-02-07" description="v1.0.8">
  ## 1.0.8

  ## New Features:

  * **Perplexity Model**: We now support [Perplexity](https://www.perplexity.ai/) as a model provider.
  * **Todoist Toolkit:** Added a toolkit for managing tasks on Todoist.
  * **JSON Reader**: Added a JSON file reader for use in knowledge bases.

  ## Improvements:

  * **LanceDb**: Implemented `name_exists` function for LanceDb.

  ## Bug Fixes:

  * **Storage growth bug:** Fixed a bug with duplication of `run_messages.messages` for every run in storage.
</Update>

<Update label="2025-02-05" description="v1.0.7">
  ## 1.0.7

  ## New Features:

  * **Google Sheets Toolkit**: Added a basic toolkit for reading, creating and updating Google sheets.
  * **Weviate Vector Store**: Added support for Weviate as a vector store.

  ## Improvements:

  * **Mistral Async**: Mistral now supports async execution via `agent.arun()` and `agent.aprint_response()`.
  * **Cohere Async**: Cohere now supports async execution via `agent.arun()` and `agent.aprint_response()`.

  ## Bug Fixes:

  * **Retriever as knowledge source**: Added small fix and examples for using the custom `retriever` parameter with an agent.
</Update>

<Update label="2025-02-05" description="v1.0.6">
  ## 1.0.6

  ## New Features:

  * **Google Maps Toolkit**: Added a rich toolkit for Google Maps that includes business discovery, directions, navigation, geocode locations, nearby places, etc.
  * **URL reader and knowledge base**: Added reader and knowledge base that can process any URL and store the text contents in the document store.

  ## Bug Fixes:

  * **Zoom tools fix:** Zoom tools updated to include the auth step and other misc fixes.
  * **Github search\_repositories pagination**: Pagination did not work correctly and this was fixed.
</Update>

<Update label="2025-02-03" description="v1.0.5">
  ## 1.0.5

  ## New Features:

  * **Gmail Tools:** Add tools for Gmail, including mail search, sending mails, etc.

  ## Improvements:

  * **Exa Toolkit Upgrade:** Added `find_similar` to `ExaTools`
  * **Claude Async:** Claude models can now be used with `await agent.aprint_response()` and `await agent.arun()`.
  * **Mistral Vision:** Mistral vision models are now supported. Various examples were added to illustrate [example](https://github.com/agno-agi/agno/blob/main/cookbook/models/mistral/image_file_input_agent.py).
</Update>

<Update label="2025-02-02" description="v1.0.4">
  ## 1.0.4

  ## Bug Fixes:

  * **Claude Tool Invocation:** Fixed issue where Claude was not working with tools that have no parameters.
</Update>

<Update label="2025-01-31" description="v1.0.3">
  ## 1.0.3

  ## Improvements:

  * **OpenAI Reasoning Parameter:** Added a reasoning parameter to OpenAI models.
</Update>

<Update label="2025-01-31" description="v1.0.2">
  ## 1.0.2

  ## Improvements:

  * **Model Client Caching:** Made all models cache the client instantiation, improving Agno agent instantiation time
  * **XTools:** Renamed `TwitterTools` to `XTools` and updated capabilities to be compatible with Twitter API v2.

  ## Bug Fixes:

  * **Agent Dataclass Compatibility:** Removed `slots=True` from the agent dataclass decorator, which was not compatible with Python \< 3.10.
  * **AzureOpenAIEmbedder:** Made `AzureOpenAIEmbedder` a dataclass to match other embedders.
</Update>

<Update label="2025-01-31" description="v1.0.1">
  ## 1.0.1

  ## Improvement:

  * **Mistral Model Caching:** Enabled caching for Mistral models.
</Update>

<Update label="2025-01-30" description="v1.0.0">
  ## 1.0.0 - Agno

  This is the major refactor from `phidata` to `agno`, released with the official launch of Agno AI.

  See the [migration guide](../how-to/phidata-to-agno) for additional guidance.

  ## Interface Changes:

  * `phi.model.x` → `agno.models.x`

  * `phi.knowledge_base.x` → `agno.knowledge.x` (applies to all knowledge bases)

  * `phi.document.reader.xxx` → `agno.document.reader.xxx_reader` (applies to all document readers)

  * All Agno toolkits are now suffixed with `Tools`. E.g. `DuckDuckGo` → `DuckDuckGoTools`

  * Multi-modal interface updates:

    * `agent.run(images=[])` and `agent.print_response(images=[])` is now of type `Image`

      ```python
      class Image(BaseModel):
          url: Optional[str] = None  # Remote location for image
          filepath: Optional[Union[Path, str]] = None  # Absolute local location for image
          content: Optional[Any] = None  # Actual image bytes content
          detail: Optional[str] = None # low, medium, high or auto (per OpenAI spec https://platform.openai.com/docs/guides/vision?lang=node#low-or-high-fidelity-image-understanding)
          id: Optional[str] = None
      ```

    * `agent.run(audio=[])` and `agent.print_response(audio=[])` is now of type `Audio`

      ```python
      class Audio(BaseModel):
          filepath: Optional[Union[Path, str]] = None  # Absolute local location for audio
          content: Optional[Any] = None  # Actual audio bytes content
          format: Optional[str] = None
      ```

    * `agent.run(video=[])` and `agent.print_response(video=[])` is now of type `Video`

      ```python
      class Video(BaseModel):
          filepath: Optional[Union[Path, str]] = None  # Absolute local location for video
          content: Optional[Any] = None  # Actual video bytes content
      ```

    * `RunResponse.images` is now a list of type `ImageArtifact`

      ```python
      class ImageArtifact(Media):
          id: str
          url: str  # Remote location for file
          alt_text: Optional[str] = None
      ```

    * `RunResponse.audio` is now a list of type `AudioArtifact`

      ```python
      class AudioArtifact(Media):
          id: str
          url: Optional[str] = None  # Remote location for file
          base64_audio: Optional[str] = None  # Base64-encoded audio data
          length: Optional[str] = None
          mime_type: Optional[str] = None
      ```

    * `RunResponse.videos` is now a list of type `VideoArtifact`

      ```python
      class VideoArtifact(Media):
          id: str
          url: str  # Remote location for file
          eta: Optional[str] = None
          length: Optional[str] = None
      ```

    * `RunResponse.response_audio` is now of type `AudioOutput`

      ```python
      class AudioOutput(BaseModel):
          id: str
          content: str  # Base64 encoded
          expires_at: int
          transcript: str
      ```

  * Models:
    * `Hermes` → `OllamaHermes`
    * `AzureOpenAIChat` → `AzureOpenAI`
    * `CohereChat` → `Cohere`
    * `DeepSeekChat` → `DeepSeek`
    * `GeminiOpenAIChat` → `GeminiOpenAI`
    * `HuggingFaceChat` → `HuggingFace`

  * Embedders now all take `id` instead of `model` as a parameter. For example

    ```python
    db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

    knowledge_base = PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        vector_db=PgVector(
            table_name="recipes",
            db_url=db_url,
            embedder=OllamaEmbedder(id="llama3.2", dimensions=3072),
        ),
    )
    knowledge_base.load(recreate=True)
    ```

  * Agent Storage class
    * `PgAgentStorage` → `PostgresDbAgentStorage`
    * `SqlAgentStorage` → `SqliteDbAgentStorage`
    * `MongoAgentStorage` → `MongoDbAgentStorage`
    * `S2AgentStorage` → `SingleStoreDbAgentStorage`

  * Workflow Storage class
    * `SqlWorkflowStorage` → `SqliteDbWorkflowStorage`
    * `PgWorkflowStorage` → `PostgresDbWorkflowStorage`
    * `MongoWorkflowStorage` → `MongoDbWorkflowStorage`

  * Knowledge Base
    * `phi.knowledge.pdf.PDFUrlKnowledgeBase` → `agno.knowledge.pdf_url.PDFUrlKnowledgeBase`
    * `phi.knowledge.csv.CSVUrlKnowledgeBase` → `agno.knowledge.csv_url.CSVUrlKnowledgeBase`

  * Readers
    * `phi.document.reader.arxiv` → `agno.document.reader.arxiv_reader`
    * `phi.document.reader.docx` → `agno.document.reader.docx_reader`
    * `phi.document.reader.json` → `agno.document.reader.json_reader`
    * `phi.document.reader.pdf` → `agno.document.reader.pdf_reader`
    * `phi.document.reader.s3.pdf` → `agno.document.reader.s3.pdf_reader`
    * `phi.document.reader.s3.text` → `agno.document.reader.s3.text_reader`
    * `phi.document.reader.text` → `agno.document.reader.text_reader`
    * `phi.document.reader.website` → `agno.document.reader.website_reader`

  ## Improvements:

  * **Dataclasses:** Changed various instances of Pydantic models to dataclasses to improve the speed.
  * Moved `Embedder` class from pydantic to data class

  ## Removals

  * Removed all references to `Assistant`
  * Removed all references to `llm`
  * Removed the `PhiTools` tool
  * On the `Agent` class, `guidelines`, `prevent_hallucinations`, `prevent_prompt_leakage`, `limit_tool_access`, and `task` has been removed. They can be incorporated into the `instructions` parameter as you see fit.

  ## Bug Fixes:

  * **Semantic Chunking:** Fixed semantic chunking by replacing `similarity_threshold` param with `threshold` param.

  ## New Features:

  * **Evals for Agents:** Introducing Evals to measure the performance, accuracy, and reliability of your Agents.
</Update>


# Agentic Chunking
Source: https://docs.agno.com/chunking/agentic-chunking



Agentic chunking is an intelligent method of splitting documents into smaller chunks by using a model to determine natural breakpoints in the text. Rather than splitting text at fixed character counts, it analyzes the content to find semantically meaningful boundaries like paragraph breaks and topic transitions.

## Usage

```python
from agno.agent import Agent
from agno.document.chunking.agentic import AgenticChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_agentic_chunking", db_url=db_url),
    chunking_strategy=AgenticChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge_base=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

## Agentic Chunking Params

<Snippet file="chunking-agentic.mdx" />


# Document Chunking
Source: https://docs.agno.com/chunking/document-chunking



Document chunking is a method of splitting documents into smaller chunks based on document structure like paragraphs and sections. It analyzes natural document boundaries rather than splitting at fixed character counts. This is useful when you want to process large documents while preserving semantic meaning and context.

## Usage

```python
from agno.agent import Agent
from agno.document.chunking.document import DocumentChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_document_chunking", db_url=db_url),
    chunking_strategy=DocumentChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge_base=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)

```

## Document Chunking Params

<Snippet file="chunking-document.mdx" />


# Fixed Size Chunking
Source: https://docs.agno.com/chunking/fixed-size-chunking



Fixed size chunking is a method of splitting documents into smaller chunks of a specified size, with optional overlap between chunks. This is useful when you want to process large documents in smaller, manageable pieces.

## Usage

```python
from agno.agent import Agent
from agno.document.chunking.fixed import FixedSizeChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_fixed_size_chunking", db_url=db_url),
    chunking_strategy=FixedSizeChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge_base=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)
```

## Fixed Size Chunking Params

<Snippet file="chunking-fixed-size.mdx" />


# Recursive Chunking
Source: https://docs.agno.com/chunking/recursive-chunking



Recursive chunking is a method of splitting documents into smaller chunks by recursively applying a chunking strategy. This is useful when you want to process large documents in smaller, manageable pieces.

```python
from agno.agent import Agent
from agno.document.chunking.recursive import RecursiveChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_recursive_chunking", db_url=db_url),
    chunking_strategy=RecursiveChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge_base=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)

```

## Recursive Chunking Params

<Snippet file="chunking-recursive.mdx" />


# Semantic Chunking
Source: https://docs.agno.com/chunking/semantic-chunking



Semantic chunking is a method of splitting documents into smaller chunks by analyzing semantic similarity between text segments using embeddings. It uses the chonkie library to identify natural breakpoints where the semantic meaning changes significantly, based on a configurable similarity threshold. This helps preserve context and meaning better than fixed-size chunking by ensuring semantically related content stays together in the same chunk, while splitting occurs at meaningful topic transitions.

```python
from agno.agent import Agent
from agno.document.chunking.semantic import SemanticChunking
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.pgvector import PgVector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    vector_db=PgVector(table_name="recipes_semantic_chunking", db_url=db_url),
    chunking_strategy=SemanticChunking(),
)
knowledge_base.load(recreate=False)  # Comment out after first run

agent = Agent(
    knowledge_base=knowledge_base,
    search_knowledge=True,
)

agent.print_response("How to make Thai curry?", markdown=True)

```

## Semantic Chunking Params

<Snippet file="chunking-semantic.mdx" />


# AWS Bedrock Embedder
Source: https://docs.agno.com/embedder/aws_bedrock



The `AwsBedrockEmbedder` class is used to embed text data into vectors using the AWS Bedrock API. By default, it uses the Cohere Embed Multilingual V3 model for generating embeddings.

# Setup

## Set your AWS credentials

```bash
export AWS_ACCESS_KEY_ID = xxx
export AWS_SECRET_ACCESS_KEY = xxx
export AWS_REGION = xxx
```

<Note>
  By default, this embedder uses the `cohere.embed-multilingual-v3` model. You must enable access to this model from the AWS Bedrock model catalog before using this embedder.
</Note>

## Run PgVector

```bash
docker run - d \
    - e POSTGRES_DB = ai \
    - e POSTGRES_USER = ai \
    - e POSTGRES_PASSWORD = ai \
    - e PGDATA = /var/lib/postgresql/data/pgdata \
    - v pgvolume: / var/lib/postgresql/data \
    - p 5532: 5432 \
    - -name pgvector \
    agnohq/pgvector: 16
```

# Usage

```python cookbook/embedders/aws_bedrock_embedder.py

# Embed sentence in database
embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)
# Print the embeddings and their dimensions
print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

# Example usage with a PDF knowledge base
knowledge_base = PDFUrlKnowledgeBase(
    urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
    reader=PDFUrlReader(
        chunk_size=2048
    ),  # Required because Cohere model has a fixed size of 2048
    vector_db=PgVector(
        table_name="recipes",
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        embedder=AwsBedrockEmbedder(),
    ),
)
knowledge_base.load(recreate=False)
```

# Params

| Parameter               | Type                       | Default                          | Description                                                                                                                   |
| ----------------------- | -------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| `id`                    | `str`                      | `"cohere.embed-multilingual-v3"` | The model ID to use. You need to enable this model in your AWS Bedrock model catalog.                                         |
| `dimensions`            | `int`                      | `1024`                           | The dimensionality of the embeddings generated by the model(1024 for Cohere models).                                          |
| `input_type`            | `str`                      | `"search_query"`                 | Prepends special tokens to differentiate types. Options: 'search\_document', 'search\_query', 'classification', 'clustering'. |
| `truncate`              | `Optional[str]`            | `None`                           | How to handle inputs longer than the maximum token length. Options: 'NONE', 'START', 'END'.                                   |
| `embedding_types`       | `Optional[List[str]]`      | `None`                           | Types of embeddings to return . Options: 'float', 'int8', 'uint8', 'binary', 'ubinary'.                                       |
| `aws_region`            | `Optional[str]`            | `None`                           | The AWS region to use. If not provided, falls back to AWS\_REGION env variable.                                               |
| `aws_access_key_id`     | `Optional[str]`            | `None`                           | The AWS access key ID. If not provided, falls back to AWS\_ACCESS\_KEY\_ID env variable.                                      |
| `aws_secret_access_key` | `Optional[str]`            | `None`                           | The AWS secret access key. If not provided, falls back to AWS\_SECRET\_ACCESS\_KEY env variable.                              |
| `session`               | `Optional[Session]`        | `None`                           | A boto3 Session object to use for authentication.                                                                             |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`                           | Additional parameters to pass to the API requests.                                                                            |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`                           | Additional parameters to pass to the boto3 client.                                                                            |
| `client`